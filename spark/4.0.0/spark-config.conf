spark.app.name=(none)
spark.driver.cores=1
spark.driver.maxResultSize=1g
spark.driver.memory=1g
spark.driver.memoryOverhead=driverMemory * spark.driver.memoryOverheadFactor, with minimum of spark.driver.minMemoryOverhead
spark.driver.minMemoryOverhead=384m
spark.driver.memoryOverheadFactor=0.10
spark.driver.resource.{resourceName}.amount=0
spark.driver.resource.{resourceName}.discoveryScript=None
spark.driver.resource.{resourceName}.vendor=None
spark.resources.discoveryPlugin=org.apache.spark.resource.ResourceDiscoveryScriptPlugin
spark.executor.memory=1g
spark.executor.pyspark.memory=Not set
spark.executor.memoryOverhead=executorMemory * spark.executor.memoryOverheadFactor, with minimum of spark.executor.minMemoryOverhead
spark.executor.minMemoryOverhead=384m
spark.executor.memoryOverheadFactor=0.10
spark.executor.resource.{resourceName}.amount=0
spark.executor.resource.{resourceName}.discoveryScript=None
spark.executor.resource.{resourceName}.vendor=None
spark.extraListeners=(none)
spark.local.dir=/tmp
spark.logConf=false
spark.master=(none)
spark.submit.deployMode=client
spark.log.callerContext=(none)
spark.log.level=(none)
spark.driver.supervise=false
spark.driver.timeout=0min
spark.driver.log.localDir=(none)
spark.driver.log.dfsDir=(none)
spark.driver.log.persistToDfs.enabled=false
spark.driver.log.layout=%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex
spark.driver.log.allowErasureCoding=false
spark.decommission.enabled=false
spark.executor.decommission.killInterval=(none)
spark.executor.decommission.forceKillTimeout=(none)
spark.executor.decommission.signal=PWR
spark.executor.maxNumFailures=numExecutors * 2, with minimum of 3
spark.executor.failuresValidityInterval=(none)
spark.driver.extraClassPath=(none)
spark.driver.defaultJavaOptions=(none)
spark.driver.extraJavaOptions=(none)
spark.driver.extraLibraryPath=(none)
spark.driver.userClassPathFirst=false
spark.executor.extraClassPath=(none)
spark.executor.defaultJavaOptions=(none)
spark.executor.extraJavaOptions=(none)
spark.executor.extraLibraryPath=(none)
spark.executor.logs.rolling.maxRetainedFiles=-1
spark.executor.logs.rolling.enableCompression=false
spark.executor.logs.rolling.maxSize=1024 * 1024
spark.executor.logs.rolling.strategy="" (disabled)
spark.executor.logs.rolling.time.interval=daily
spark.executor.userClassPathFirst=false
spark.executorEnv.[EnvironmentVariableName]=(none)
spark.redaction.regex=(?i)secret|password|token|access[.]?key
spark.redaction.string.regex=(none)
spark.python.profile=false
spark.python.profile.dump=(none)
spark.python.worker.memory=512m
spark.python.worker.reuse=true
spark.files=
spark.submit.pyFiles=
spark.jars=
spark.jars.packages=
spark.jars.excludes=
spark.jars.ivy=
spark.jars.ivySettings=
spark.jars.repositories=
spark.archives=
spark.pyspark.driver.python=
spark.pyspark.python=
spark.reducer.maxSizeInFlight=48m
spark.reducer.maxReqsInFlight=Int.MaxValue
spark.reducer.maxBlocksInFlightPerAddress=Int.MaxValue
spark.shuffle.compress=true
spark.shuffle.file.buffer=32k
spark.shuffle.file.merge.buffer=32k
spark.shuffle.unsafe.file.output.buffer=32k
spark.shuffle.localDisk.file.output.buffer=32k
spark.shuffle.spill.diskWriteBufferSize=1024 * 1024
spark.shuffle.io.maxRetries=3
spark.shuffle.io.numConnectionsPerPeer=1
spark.shuffle.io.preferDirectBufs=true
spark.shuffle.io.retryWait=5s
spark.shuffle.io.backLog=-1
spark.shuffle.io.connectionTimeout=value of spark.network.timeout
spark.shuffle.io.connectionCreationTimeout=value of spark.shuffle.io.connectionTimeout
spark.shuffle.service.enabled=false
spark.shuffle.service.port=7337
spark.shuffle.service.name=spark_shuffle
spark.shuffle.service.index.cache.size=100m
spark.shuffle.service.removeShuffle=true
spark.shuffle.maxChunksBeingTransferred=Long.MAX_VALUE
spark.shuffle.sort.bypassMergeThreshold=200
spark.shuffle.sort.io.plugin.class=org.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO
spark.shuffle.spill.compress=true
spark.shuffle.accurateBlockThreshold=100 * 1024 * 1024
spark.shuffle.accurateBlockSkewedFactor=-1.0
spark.shuffle.registration.timeout=5000
spark.shuffle.registration.maxAttempts=3
spark.shuffle.reduceLocality.enabled=true
spark.shuffle.mapOutput.minSizeForBroadcast=512k
spark.shuffle.detectCorrupt=true
spark.shuffle.detectCorrupt.useExtraMemory=false
spark.shuffle.useOldFetchProtocol=false
spark.shuffle.readHostLocalDisk=true
spark.files.io.connectionTimeout=value of spark.network.timeout
spark.files.io.connectionCreationTimeout=value of spark.files.io.connectionTimeout
spark.shuffle.checksum.enabled=true
spark.shuffle.checksum.algorithm=ADLER32
spark.shuffle.service.fetch.rdd.enabled=false
spark.shuffle.service.db.enabled=true
spark.shuffle.service.db.backend=ROCKSDB
spark.eventLog.logBlockUpdates.enabled=false
spark.eventLog.longForm.enabled=false
spark.eventLog.compress=true
spark.eventLog.compression.codec=zstd
spark.eventLog.erasureCoding.enabled=false
spark.eventLog.dir=file:///tmp/spark-events
spark.eventLog.enabled=false
spark.eventLog.overwrite=false
spark.eventLog.buffer.kb=100k
spark.eventLog.rolling.enabled=false
spark.eventLog.rolling.maxFileSize=128m
spark.ui.dagGraph.retainedRootRDDs=Int.MaxValue
spark.ui.groupSQLSubExecutionEnabled=true
spark.ui.enabled=true
spark.ui.store.path=None
spark.ui.killEnabled=true
spark.ui.threadDumpsEnabled=true
spark.ui.threadDump.flamegraphEnabled=true
spark.ui.heapHistogramEnabled=true
spark.ui.liveUpdate.period=100ms
spark.ui.liveUpdate.minFlushPeriod=1s
spark.ui.port=4040
spark.ui.retainedJobs=1000
spark.ui.retainedStages=1000
spark.ui.retainedTasks=100000
spark.ui.reverseProxy=false
spark.ui.reverseProxyUrl=
spark.ui.proxyRedirectUri=
spark.ui.showConsoleProgress=false
spark.ui.consoleProgress.update.interval=200
spark.ui.custom.executor.log.url=(none)
spark.ui.prometheus.enabled=true
spark.worker.ui.retainedExecutors=1000
spark.worker.ui.retainedDrivers=1000
spark.sql.ui.retainedExecutions=1000
spark.streaming.ui.retainedBatches=1000
spark.ui.retainedDeadExecutors=100
spark.ui.filters=None
spark.ui.requestHeaderSize=8k
spark.ui.timelineEnabled=true
spark.ui.timeline.executors.maximum=250
spark.ui.timeline.jobs.maximum=500
spark.ui.timeline.stages.maximum=500
spark.ui.timeline.tasks.maximum=1000
spark.appStatusStore.diskStoreDir=None
spark.broadcast.compress=true
spark.checkpoint.dir=(none)
spark.checkpoint.compress=false
spark.io.compression.codec=lz4
spark.io.compression.lz4.blockSize=32k
spark.io.compression.snappy.blockSize=32k
spark.io.compression.zstd.level=1
spark.io.compression.zstd.bufferSize=32k
spark.io.compression.zstd.bufferPool.enabled=true
spark.io.compression.zstd.workers=0
spark.io.compression.lzf.parallel.enabled=false
spark.kryo.classesToRegister=(none)
spark.kryo.referenceTracking=true
spark.kryo.registrationRequired=false
spark.kryo.registrator=(none)
spark.kryo.unsafe=true
spark.kryoserializer.buffer.max=64m
spark.kryoserializer.buffer=64k
spark.rdd.compress=false
spark.serializer=org.apache.spark.serializer. JavaSerializer
spark.serializer.objectStreamReset=100
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5
spark.memory.offHeap.enabled=false
spark.memory.offHeap.size=0
spark.storage.unrollMemoryThreshold=1024 * 1024
spark.storage.replication.proactive=true
spark.storage.localDiskByExecutors.cacheSize=1000
spark.cleaner.periodicGC.interval=30min
spark.cleaner.referenceTracking=true
spark.cleaner.referenceTracking.blocking=true
spark.cleaner.referenceTracking.blocking.shuffle=false
spark.cleaner.referenceTracking.cleanCheckpoints=false
spark.broadcast.blockSize=4m
spark.broadcast.checksum=true
spark.broadcast.UDFCompressionThreshold=1 * 1024 * 1024
spark.executor.cores=1 in YARN mode, all the available cores on the worker in standalone mode.
spark.default.parallelism=For distributed shuffle operations like reduceByKey and join, the largest number of partitions in a parent RDD. For operations like parallelize with no parent RDDs, it depends on the cluster manager: Local mode: number of cores on the local machine Others: total number of cores on all executor nodes or 2, whichever is larger
spark.executor.heartbeatInterval=10s
spark.files.fetchTimeout=60s
spark.files.useFetchCache=true
spark.files.overwrite=false
spark.files.ignoreCorruptFiles=false
spark.files.ignoreMissingFiles=false
spark.files.maxPartitionBytes=134217728 (128 MiB)
spark.files.openCostInBytes=4194304 (4 MiB)
spark.hadoop.cloneConf=false
spark.hadoop.validateOutputSpecs=true
spark.storage.memoryMapThreshold=2m
spark.storage.decommission.enabled=false
spark.storage.decommission.shuffleBlocks.enabled=true
spark.storage.decommission.shuffleBlocks.maxThreads=8
spark.storage.decommission.rddBlocks.enabled=true
spark.storage.decommission.fallbackStorage.path=(none)
spark.storage.decommission.fallbackStorage.cleanUp=false
spark.storage.decommission.shuffleBlocks.maxDiskSize=(none)
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1
spark.eventLog.logStageExecutorMetrics=false
spark.executor.processTreeMetrics.enabled=false
spark.executor.metrics.pollingInterval=0
spark.eventLog.gcMetrics.youngGenerationGarbageCollectors=Copy,PS Scavenge,ParNew,G1 Young Generation
spark.eventLog.gcMetrics.oldGenerationGarbageCollectors=MarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation
spark.executor.metrics.fileSystemSchemes=file,hdfs
spark.rpc.message.maxSize=128
spark.blockManager.port=(random)
spark.driver.blockManager.port=(value of spark.blockManager.port)
spark.driver.bindAddress=(value of spark.driver.host)
spark.driver.host=(local hostname)
spark.driver.port=(random)
spark.rpc.io.backLog=64
spark.network.timeout=120s
spark.network.timeoutInterval=60s
spark.network.io.preferDirectBufs=true
spark.port.maxRetries=16
spark.rpc.askTimeout=spark.network.timeout
spark.rpc.lookupTimeout=120s
spark.network.maxRemoteBlockSizeFetchToMem=200m
spark.rpc.io.connectionTimeout=value of spark.network.timeout
spark.rpc.io.connectionCreationTimeout=value of spark.rpc.io.connectionTimeout
spark.cores.max=(not set)
spark.locality.wait=3s
spark.locality.wait.node=spark.locality.wait
spark.locality.wait.process=spark.locality.wait
spark.locality.wait.rack=spark.locality.wait
spark.scheduler.maxRegisteredResourcesWaitingTime=30s
spark.scheduler.minRegisteredResourcesRatio=0.8 for KUBERNETES mode; 0.8 for YARN mode; 0.0 for standalone mode
spark.scheduler.mode=FIFO
spark.scheduler.revive.interval=1s
spark.scheduler.listenerbus.eventqueue.capacity=10000
spark.scheduler.listenerbus.eventqueue.shared.capacity=spark.scheduler.listenerbus.eventqueue.capacity
spark.scheduler.listenerbus.eventqueue.appStatus.capacity=spark.scheduler.listenerbus.eventqueue.capacity
spark.scheduler.listenerbus.eventqueue.executorManagement.capacity=spark.scheduler.listenerbus.eventqueue.capacity
spark.scheduler.listenerbus.eventqueue.eventLog.capacity=spark.scheduler.listenerbus.eventqueue.capacity
spark.scheduler.listenerbus.eventqueue.streams.capacity=spark.scheduler.listenerbus.eventqueue.capacity
spark.scheduler.resource.profileMergeConflicts=false
spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout=120s
spark.standalone.submit.waitAppCompletion=false
spark.excludeOnFailure.enabled=false
spark.excludeOnFailure.application.enabled=false
spark.excludeOnFailure.taskAndStage.enabled=false
spark.excludeOnFailure.timeout=1h
spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor=1
spark.excludeOnFailure.task.maxTaskAttemptsPerNode=2
spark.excludeOnFailure.stage.maxFailedTasksPerExecutor=2
spark.excludeOnFailure.stage.maxFailedExecutorsPerNode=2
spark.excludeOnFailure.application.maxFailedTasksPerExecutor=2
spark.excludeOnFailure.application.maxFailedExecutorsPerNode=2
spark.excludeOnFailure.killExcludedExecutors=false
spark.excludeOnFailure.application.fetchFailure.enabled=false
spark.speculation=false
spark.speculation.interval=100ms
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.speculation.minTaskRuntime=100ms
spark.speculation.task.duration.threshold=None
spark.speculation.efficiency.processRateMultiplier=0.75
spark.speculation.efficiency.longRunTaskFactor=2
spark.speculation.efficiency.enabled=true
spark.task.cpus=1
spark.task.resource.{resourceName}.amount=1
spark.task.maxFailures=4
spark.task.reaper.enabled=false
spark.task.reaper.pollingInterval=10s
spark.task.reaper.threadDump=true
spark.task.reaper.killTimeout=-1
spark.stage.maxConsecutiveAttempts=4
spark.stage.ignoreDecommissionFetchFailure=true
spark.barrier.sync.timeout=365d
spark.scheduler.barrier.maxConcurrentTasksCheck.interval=15s
spark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures=40
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=infinity
spark.dynamicAllocation.initialExecutors=spark.dynamicAllocation.minExecutors
spark.dynamicAllocation.maxExecutors=infinity
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.executorAllocationRatio=1
spark.dynamicAllocation.schedulerBacklogTimeout=1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=schedulerBacklogTimeout
spark.dynamicAllocation.shuffleTracking.enabled=true
spark.dynamicAllocation.shuffleTracking.timeout=infinity
spark.{driver|executor}.rpc.io.serverThreads=Fall back on spark.rpc.io.serverThreads
spark.{driver|executor}.rpc.io.clientThreads=Fall back on spark.rpc.io.clientThreads
spark.{driver|executor}.rpc.netty.dispatcher.numThreads=Fall back on spark.rpc.netty.dispatcher.numThreads
spark.api.mode=classic
spark.connect.grpc.binding.address=(none)
spark.connect.grpc.binding.port=15002
spark.connect.grpc.port.maxRetries=0
spark.connect.grpc.interceptor.classes=(none)
spark.connect.grpc.arrow.maxBatchSize=4m
spark.connect.grpc.maxInboundMessageSize=134217728
spark.connect.extensions.relation.classes=(none)
spark.connect.extensions.expression.classes=(none)
spark.connect.extensions.command.classes=(none)
spark.connect.ml.backend.classes=(none)
spark.connect.jvmStacktrace.maxSize=1024
spark.sql.connect.ui.retainedSessions=200
spark.sql.connect.ui.retainedStatements=200
spark.sql.connect.enrichError.enabled=true
spark.sql.connect.serverStacktrace.enabled=true
spark.connect.grpc.maxMetadataSize=1024
spark.connect.progress.reportInterval=2s
spark.sql.adaptive.advisoryPartitionSizeInBytes=(value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize)
spark.sql.adaptive.autoBroadcastJoinThreshold=(none)
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.initialPartitionNum=(none)
spark.sql.adaptive.coalescePartitions.minPartitionSize=1MB
spark.sql.adaptive.coalescePartitions.parallelismFirst=true
spark.sql.adaptive.customCostEvaluatorClass=(none)
spark.sql.adaptive.enabled=true
spark.sql.adaptive.forceOptimizeSkewedJoin=false
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold=0b
spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled=true
spark.sql.adaptive.optimizer.excludedRules=(none)
spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor=0.2
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.skewJoin.skewedPartitionFactor=5.0
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=256MB
spark.sql.allowNamedFunctionArguments=true
spark.sql.ansi.doubleQuotedIdentifiers=false
spark.sql.ansi.enabled=true
spark.sql.ansi.enforceReservedKeywords=false
spark.sql.ansi.relationPrecedence=false
spark.sql.autoBroadcastJoinThreshold=10MB
spark.sql.avro.compression.codec=snappy
spark.sql.avro.deflate.level=-1
spark.sql.avro.filterPushdown.enabled=true
spark.sql.avro.xz.level=6
spark.sql.avro.zstandard.bufferPool.enabled=false
spark.sql.avro.zstandard.level=3
spark.sql.binaryOutputStyle=(none)
spark.sql.broadcastTimeout=300
spark.sql.bucketing.coalesceBucketsInJoin.enabled=false
spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio=4
spark.sql.catalog.spark_catalog=builtin
spark.sql.cbo.enabled=false
spark.sql.cbo.joinReorder.dp.star.filter=false
spark.sql.cbo.joinReorder.dp.threshold=12
spark.sql.cbo.joinReorder.enabled=false
spark.sql.cbo.planStats.enabled=false
spark.sql.cbo.starSchemaDetection=false
spark.sql.charAsVarchar=false
spark.sql.chunkBase64String.enabled=true
spark.sql.cli.print.header=false
spark.sql.columnNameOfCorruptRecord=_corrupt_record
spark.sql.csv.filterPushdown.enabled=true
spark.sql.datetime.java8API.enabled=false
spark.sql.debug.maxToStringFields=25
spark.sql.defaultCacheStorageLevel=MEMORY_AND_DISK
spark.sql.defaultCatalog=spark_catalog
spark.sql.error.messageFormat=PRETTY
spark.sql.execution.arrow.enabled=false
spark.sql.execution.arrow.fallback.enabled=true
spark.sql.execution.arrow.localRelationThreshold=48MB
spark.sql.execution.arrow.maxRecordsPerBatch=10000
spark.sql.execution.arrow.pyspark.enabled=(value of spark.sql.execution.arrow.enabled)
spark.sql.execution.arrow.pyspark.fallback.enabled=(value of spark.sql.execution.arrow.fallback.enabled)
spark.sql.execution.arrow.pyspark.selfDestruct.enabled=false
spark.sql.execution.arrow.sparkr.enabled=false
spark.sql.execution.arrow.transformWithStateInPandas.maxRecordsPerBatch=10000
spark.sql.execution.arrow.useLargeVarTypes=false
spark.sql.execution.interruptOnCancel=true
spark.sql.execution.pandas.inferPandasDictAsMap=false
spark.sql.execution.pandas.structHandlingMode=legacy
spark.sql.execution.pandas.udf.buffer.size=(value of spark.buffer.size)
spark.sql.execution.pyspark.udf.faulthandler.enabled=(value of spark.python.worker.faulthandler.enabled)
spark.sql.execution.pyspark.udf.hideTraceback.enabled=false
spark.sql.execution.pyspark.udf.idleTimeoutSeconds=(value of spark.python.worker.idleTimeoutSeconds)
spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled=true
spark.sql.execution.python.udf.buffer.size=(value of spark.buffer.size)
spark.sql.execution.python.udf.maxRecordsPerBatch=100
spark.sql.execution.pythonUDF.arrow.concurrency.level=(none)
spark.sql.execution.pythonUDF.arrow.enabled=false
spark.sql.execution.pythonUDTF.arrow.enabled=false
spark.sql.execution.topKSortFallbackThreshold=2147483632
spark.sql.extendedExplainProviders=(none)
spark.sql.files.ignoreCorruptFiles=false
spark.sql.files.ignoreInvalidPartitionPaths=false
spark.sql.files.ignoreMissingFiles=false
spark.sql.files.maxPartitionBytes=128MB
spark.sql.files.maxPartitionNum=(none)
spark.sql.files.maxRecordsPerFile=0
spark.sql.files.minPartitionNum=(none)
spark.sql.function.concatBinaryAsString=false
spark.sql.function.eltOutputAsString=false
spark.sql.groupByAliases=true
spark.sql.groupByOrdinal=true
spark.sql.hive.convertInsertingPartitionedTable=true
spark.sql.hive.convertInsertingUnpartitionedTable=true
spark.sql.hive.convertMetastoreCtas=true
spark.sql.hive.convertMetastoreInsertDir=true
spark.sql.hive.convertMetastoreOrc=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.convertMetastoreParquet.mergeSchema=false
spark.sql.hive.dropPartitionByName.enabled=false
spark.sql.hive.filesourcePartitionFileCacheSize=262144000
spark.sql.hive.manageFilesourcePartitions=true
spark.sql.hive.metastorePartitionPruning=true
spark.sql.hive.metastorePartitionPruningFallbackOnException=false
spark.sql.hive.metastorePartitionPruningFastFallback=false
spark.sql.hive.thriftServer.async=true
spark.sql.icu.caseMappings.enabled=true
spark.sql.inMemoryColumnarStorage.batchSize=10000
spark.sql.inMemoryColumnarStorage.compressed=true
spark.sql.inMemoryColumnarStorage.enableVectorizedReader=true
spark.sql.inMemoryColumnarStorage.hugeVectorReserveRatio=1.2
spark.sql.inMemoryColumnarStorage.hugeVectorThreshold=-1b
spark.sql.json.filterPushdown.enabled=true
spark.sql.json.useUnsafeRow=false
spark.sql.jsonGenerator.ignoreNullFields=true
spark.sql.leafNodeDefaultParallelism=(none)
spark.sql.mapKeyDedupPolicy=EXCEPTION
spark.sql.maven.additionalRemoteRepositories=https://maven-central.storage-download.googleapis.com/maven2/
spark.sql.maxMetadataStringLength=100
spark.sql.maxPlanStringLength=2147483632
spark.sql.maxSinglePartitionBytes=128m
spark.sql.operatorPipeSyntaxEnabled=true
spark.sql.optimizer.avoidCollapseUDFWithExpensiveExpr=true
spark.sql.optimizer.collapseProjectAlwaysInline=false
spark.sql.optimizer.dynamicPartitionPruning.enabled=true
spark.sql.optimizer.enableCsvExpressionOptimization=true
spark.sql.optimizer.enableJsonExpressionOptimization=true
spark.sql.optimizer.excludedRules=(none)
spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold=10GB
spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold=10MB
spark.sql.optimizer.runtime.bloomFilter.enabled=true
spark.sql.optimizer.runtime.bloomFilter.expectedNumItems=1000000
spark.sql.optimizer.runtime.bloomFilter.maxNumBits=67108864
spark.sql.optimizer.runtime.bloomFilter.maxNumItems=4000000
spark.sql.optimizer.runtime.bloomFilter.numBits=8388608
spark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled=true
spark.sql.optimizer.runtimeFilter.number.threshold=10
spark.sql.orc.aggregatePushdown=false
spark.sql.orc.columnarReaderBatchSize=4096
spark.sql.orc.columnarWriterBatchSize=1024
spark.sql.orc.compression.codec=zstd
spark.sql.orc.enableNestedColumnVectorizedReader=true
spark.sql.orc.enableVectorizedReader=true
spark.sql.orc.filterPushdown=true
spark.sql.orc.mergeSchema=false
spark.sql.orderByOrdinal=true
spark.sql.parquet.aggregatePushdown=false
spark.sql.parquet.binaryAsString=false
spark.sql.parquet.columnarReaderBatchSize=4096
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.enableNestedColumnVectorizedReader=true
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.fieldId.read.enabled=false
spark.sql.parquet.fieldId.read.ignoreMissing=false
spark.sql.parquet.fieldId.write.enabled=true
spark.sql.parquet.filterPushdown=true
spark.sql.parquet.inferTimestampNTZ.enabled=true
spark.sql.parquet.int96AsTimestamp=true
spark.sql.parquet.int96TimestampConversion=false
spark.sql.parquet.mergeSchema=false
spark.sql.parquet.outputTimestampType=INT96
spark.sql.parquet.recordLevelFilter.enabled=false
spark.sql.parquet.respectSummaryFiles=false
spark.sql.parquet.writeLegacyFormat=false
spark.sql.parser.quotedRegexColumnNames=false
spark.sql.pivotMaxValues=10000
spark.sql.planner.pythonExecution.memory=(none)
spark.sql.preserveCharVarcharTypeInfo=false
spark.sql.pyspark.inferNestedDictAsStruct.enabled=false
spark.sql.pyspark.jvmStacktrace.enabled=false
spark.sql.pyspark.plotting.max_rows=1000
spark.sql.pyspark.udf.profiler=(none)
spark.sql.readSideCharPadding=true
spark.sql.redaction.options.regex=(?i)url
spark.sql.redaction.string.regex=(value of spark.redaction.string.regex)
spark.sql.repl.eagerEval.enabled=false
spark.sql.repl.eagerEval.maxNumRows=20
spark.sql.repl.eagerEval.truncate=20
spark.sql.scripting.enabled=false
spark.sql.session.localRelationCacheThreshold=67108864
spark.sql.session.timeZone=(value of local timezone)
spark.sql.shuffle.partitions=200
spark.sql.shuffleDependency.fileCleanup.enabled=false
spark.sql.shuffleDependency.skipMigration.enabled=false
spark.sql.shuffledHashJoinFactor=3
spark.sql.sources.bucketing.autoBucketedScan.enabled=true
spark.sql.sources.bucketing.enabled=true
spark.sql.sources.bucketing.maxBuckets=100000
spark.sql.sources.default=parquet
spark.sql.sources.parallelPartitionDiscovery.threshold=32
spark.sql.sources.partitionColumnTypeInference.enabled=true
spark.sql.sources.partitionOverwriteMode=STATIC
spark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled=false
spark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled=false
spark.sql.sources.v2.bucketing.enabled=false
spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled=false
spark.sql.sources.v2.bucketing.partition.filter.enabled=false
spark.sql.sources.v2.bucketing.pushPartValues.enabled=true
spark.sql.sources.v2.bucketing.shuffle.enabled=false
spark.sql.sources.v2.bucketing.sorting.enabled=false
spark.sql.stackTracesInDataFrameContext=1
spark.sql.statistics.fallBackToHdfs=false
spark.sql.statistics.histogram.enabled=false
spark.sql.statistics.size.autoUpdate.enabled=false
spark.sql.statistics.updatePartitionStatsInAnalyzeTable.enabled=false
spark.sql.storeAssignmentPolicy=ANSI
spark.sql.streaming.checkpointLocation=(none)
spark.sql.streaming.continuous.epochBacklogQueueSize=10000
spark.sql.streaming.disabledV2Writers=
spark.sql.streaming.fileSource.cleaner.numThreads=1
spark.sql.streaming.forceDeleteTempCheckpointLocation=false
spark.sql.streaming.metricsEnabled=false
spark.sql.streaming.multipleWatermarkPolicy=min
spark.sql.streaming.noDataMicroBatches.enabled=true
spark.sql.streaming.numRecentProgressUpdates=100
spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition=false
spark.sql.streaming.stateStore.encodingFormat=unsaferow
spark.sql.streaming.stateStore.stateSchemaCheck=true
spark.sql.streaming.stopActiveRunOnRestart=true
spark.sql.streaming.stopTimeout=0
spark.sql.streaming.transformWithState.stateSchemaVersion=3
spark.sql.thriftServer.interruptOnCancel=(value of spark.sql.execution.interruptOnCancel)
spark.sql.thriftServer.queryTimeout=0ms
spark.sql.thriftserver.scheduler.pool=(none)
spark.sql.thriftserver.ui.retainedSessions=200
spark.sql.thriftserver.ui.retainedStatements=200
spark.sql.timeTravelTimestampKey=timestampAsOf
spark.sql.timeTravelVersionKey=versionAsOf
spark.sql.timestampType=TIMESTAMP_LTZ
spark.sql.transposeMaxValues=500
spark.sql.tvf.allowMultipleTableArguments.enabled=false
spark.sql.ui.explainMode=formatted
spark.sql.variable.substitute=true
spark.sql.cache.serializer=org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer
spark.sql.catalog.spark_catalog.defaultDatabase=default
spark.sql.event.truncate.length=2147483647
spark.sql.extensions=(none)
spark.sql.extensions.test.loadFromCp=true
spark.sql.hive.metastore.barrierPrefixes=
spark.sql.hive.metastore.jars=builtin
spark.sql.hive.metastore.jars.path=
spark.sql.hive.metastore.sharedPrefixes=com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc
spark.sql.hive.metastore.version=2.3.10
spark.sql.hive.thriftServer.singleSession=false
spark.sql.hive.version=2.3.10
spark.sql.metadataCacheTTLSeconds=-1000ms
spark.sql.queryExecutionListeners=(none)
spark.sql.sources.disabledJdbcConnProviderList=
spark.sql.streaming.streamingQueryListeners=(none)
spark.sql.streaming.ui.enabled=true
spark.sql.streaming.ui.retainedProgressUpdates=100
spark.sql.streaming.ui.retainedQueries=100
spark.sql.ui.retainedExecutions=1000
spark.sql.warehouse.dir=(value of $PWD/spark-warehouse)
spark.streaming.backpressure.enabled=false
spark.streaming.backpressure.initialRate=not set
spark.streaming.blockInterval=200ms
spark.streaming.receiver.maxRate=not set
spark.streaming.receiver.writeAheadLog.enable=false
spark.streaming.unpersist=true
spark.streaming.stopGracefullyOnShutdown=false
spark.streaming.kafka.maxRatePerPartition=not set
spark.streaming.kafka.minRatePerPartition=1
spark.streaming.ui.retainedBatches=1000
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=false
spark.streaming.receiver.writeAheadLog.closeFileAfterWrite=false
spark.r.numRBackendThreads=2
spark.r.command=Rscript
spark.r.driver.command=spark.r.command
spark.r.shell.command=R
spark.r.backendConnectionTimeout=6000
spark.r.heartBeatInterval=100
spark.graphx.pregel.checkpointInterval=-1
spark.shuffle.push.server.mergedShuffleFileManagerImpl=org.apache.spark.network.shuffle. NoOpMergedShuffleFileManager
spark.shuffle.push.server.minChunkSizeInMergedShuffleFile=2m
spark.shuffle.push.server.mergedIndexCacheSize=100m
spark.shuffle.push.enabled=false
spark.shuffle.push.finalize.timeout=10s
spark.shuffle.push.maxRetainedMergerLocations=500
spark.shuffle.push.mergersMinThresholdRatio=0.05
spark.shuffle.push.mergersMinStaticThreshold=5
spark.shuffle.push.numPushThreads=(none)
spark.shuffle.push.maxBlockSizeToPush=1m
spark.shuffle.push.maxBlockBatchSize=3m
spark.shuffle.push.merge.finalizeThreads=8
spark.shuffle.push.minShuffleSizeToWait=500m
spark.shuffle.push.minCompletedPushRatio=1.0
spark.yarn.shuffle.server.recovery.disabled=false
spark.authenticate=false
spark.authenticate.secret=None
spark.authenticate.secret.file=None
spark.authenticate.secret.driver.file=The value of spark.authenticate.secret.file
spark.authenticate.secret.executor.file=The value of spark.authenticate.secret.file
spark.network.crypto.enabled=false
spark.network.crypto.cipher=AES/CTR/NoPadding
spark.network.crypto.authEngineVersion=1
spark.network.crypto.config.*=None
spark.network.crypto.saslFallback=true
spark.authenticate.enableSaslEncryption=false
spark.network.sasl.serverAlwaysEncrypt=false
spark.io.encryption.enabled=false
spark.io.encryption.keySizeBits=128
spark.io.encryption.keygen.algorithm=HmacSHA1
spark.io.encryption.commons.config.*=None
spark.ui.allowFramingFrom=SAMEORIGIN
spark.ui.filters=None
spark.acls.enable=false
spark.admin.acls=None
spark.admin.acls.groups=None
spark.modify.acls=None
spark.modify.acls.groups=None
spark.ui.view.acls=None
spark.ui.view.acls.groups=None
spark.user.groups.mapping=org.apache.spark.security.ShellBasedGroupsMappingProvider
spark.history.ui.acls.enable=false
spark.history.ui.admin.acls=None
spark.history.ui.admin.acls.groups=None
${ns}.enabled=false
${ns}.port=None
${ns}.enabledAlgorithms=None
${ns}.keyPassword=None
${ns}.keyStore=None
${ns}.keyStorePassword=None
${ns}.keyStoreType=JKS
${ns}.protocol=None
${ns}.needClientAuth=false
${ns}.trustStore=None
${ns}.trustStorePassword=None
${ns}.trustStoreType=JKS
${ns}.openSSLEnabled=false
${ns}.privateKey=None
${ns}.privateKeyPassword=None
${ns}.certChain=None
${ns}.trustStoreReloadingEnabled=false
${ns}.trustStoreReloadIntervalMs=10000
spark.ui.xXssProtection=1; mode=block
spark.ui.xContentTypeOptions.enabled=true
spark.ui.strictTransportSecurity=None
spark.security.credentials.${service}.enabled=true
spark.kerberos.access.hadoopFileSystems=(none)
spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory=file:/tmp/spark-events
spark.history.fs.update.interval=10s
spark.history.retainedApplications=50
spark.history.ui.maxApplications=Int.MaxValue
spark.history.ui.port=18080
spark.history.kerberos.enabled=false
spark.history.kerberos.principal=(none)
spark.history.kerberos.keytab=(none)
spark.history.fs.cleaner.enabled=false
spark.history.fs.cleaner.interval=1d
spark.history.fs.cleaner.maxAge=7d
spark.history.fs.cleaner.maxNum=Int.MaxValue
spark.history.fs.endEventReparseChunkSize=1m
spark.history.fs.inProgressOptimization.enabled=true
spark.history.fs.driverlog.cleaner.enabled=spark.history.fs.cleaner.enabled
spark.history.fs.driverlog.cleaner.interval=spark.history.fs.cleaner.interval
spark.history.fs.driverlog.cleaner.maxAge=spark.history.fs.cleaner.maxAge
spark.history.fs.numReplayThreads=25% of available cores
spark.history.store.maxDiskUsage=10g
spark.history.store.path=(none)
spark.history.store.serializer=JSON
spark.history.custom.executor.log.url=(none)
spark.history.custom.executor.log.url.applyIncompleteApplication=true
spark.history.fs.eventLog.rolling.maxFilesToRetain=Int.MaxValue
spark.history.store.hybridStore.enabled=false
spark.history.store.hybridStore.maxMemoryUsage=2g
spark.history.store.hybridStore.diskBackend=ROCKSDB
spark.history.fs.update.batchSize=Int.MaxValue
spark.yarn.am.memory=512m
spark.yarn.am.resource.{resource-type}.amount=(none)
spark.yarn.applicationType=SPARK
spark.yarn.driver.resource.{resource-type}.amount=(none)
spark.yarn.executor.resource.{resource-type}.amount=(none)
spark.yarn.resourceGpuDeviceName=yarn.io/gpu
spark.yarn.resourceFpgaDeviceName=yarn.io/fpga
spark.yarn.am.cores=1
spark.yarn.am.waitTime=100s
spark.yarn.submit.file.replication=The default HDFS replication (usually 3)
spark.yarn.stagingDir=Current user's home directory in the filesystem
spark.yarn.preserve.staging.files=false
spark.yarn.scheduler.heartbeat.interval-ms=3000
spark.yarn.scheduler.initial-allocation.interval=200ms
spark.yarn.historyServer.address=(none)
spark.yarn.dist.archives=(none)
spark.yarn.dist.files=(none)
spark.yarn.dist.jars=(none)
spark.yarn.dist.forceDownloadSchemes=(none)
spark.executor.instances=2
spark.yarn.am.memoryOverhead=AM memory * 0.10, with minimum of 384
spark.yarn.queue=default
spark.yarn.jars=(none)
spark.yarn.archive=(none)
spark.yarn.appMasterEnv.[EnvironmentVariableName]=(none)
spark.yarn.containerLauncherMaxThreads=25
spark.yarn.am.extraJavaOptions=(none)
spark.yarn.am.extraLibraryPath=(none)
spark.yarn.populateHadoopClasspath=For with-hadoop Spark distribution, this is set to false; for no-hadoop distribution, this is set to true.
spark.yarn.maxAppAttempts=yarn.resourcemanager.am.max-attempts in YARN
spark.yarn.am.attemptFailuresValidityInterval=(none)
spark.yarn.am.clientModeTreatDisconnectAsFailed=false
spark.yarn.am.clientModeExitOnError=false
spark.yarn.am.tokenConfRegex=(none)
spark.yarn.submit.waitAppCompletion=true
spark.yarn.am.nodeLabelExpression=(none)
spark.yarn.executor.nodeLabelExpression=(none)
spark.yarn.tags=(none)
spark.yarn.priority=(none)
spark.yarn.config.gatewayPath=(none)
spark.yarn.config.replacementPath=(none)
spark.yarn.rolledLog.includePattern=(none)
spark.yarn.rolledLog.excludePattern=(none)
spark.yarn.executor.launch.excludeOnFailure.enabled=false
spark.yarn.exclude.nodes=(none)
spark.yarn.metrics.namespace=(none)
spark.yarn.report.interval=1s
spark.yarn.report.loggingFrequency=30
spark.yarn.clientLaunchMonitorInterval=1s
spark.yarn.includeDriverLogsLink=false
spark.yarn.unmanagedAM.enabled=false
spark.yarn.shuffle.server.recovery.disabled=false
spark.kerberos.keytab=(none)
spark.kerberos.principal=(none)
spark.yarn.kerberos.relogin.period=1m
spark.yarn.kerberos.renewal.excludeHadoopFileSystems=(none)
spark.yarn.shuffle.stopOnFailure=false
spark.yarn.shuffle.service.metrics.namespace=sparkShuffleService
spark.yarn.shuffle.service.logs.namespace=(not set)
spark.shuffle.service.db.backend=ROCKSDB
spark.kubernetes.context=(none)
spark.kubernetes.driver.master=https://kubernetes.default.svc
spark.kubernetes.namespace=default
spark.kubernetes.container.image=(none)
spark.kubernetes.driver.container.image=(value of spark.kubernetes.container.image)
spark.kubernetes.executor.container.image=(value of spark.kubernetes.container.image)
spark.kubernetes.container.image.pullPolicy=IfNotPresent
spark.kubernetes.container.image.pullSecrets=
spark.kubernetes.allocation.batch.size=10
spark.kubernetes.allocation.batch.delay=1s
spark.kubernetes.jars.avoidDownloadSchemes=(none)
spark.kubernetes.authenticate.submission.caCertFile=(none)
spark.kubernetes.authenticate.submission.clientKeyFile=(none)
spark.kubernetes.authenticate.submission.clientCertFile=(none)
spark.kubernetes.authenticate.submission.oauthToken=(none)
spark.kubernetes.authenticate.submission.oauthTokenFile=(none)
spark.kubernetes.authenticate.driver.caCertFile=(none)
spark.kubernetes.authenticate.driver.clientKeyFile=(none)
spark.kubernetes.authenticate.driver.clientCertFile=(none)
spark.kubernetes.authenticate.driver.oauthToken=(none)
spark.kubernetes.authenticate.driver.oauthTokenFile=(none)
spark.kubernetes.authenticate.driver.mounted.caCertFile=(none)
spark.kubernetes.authenticate.driver.mounted.clientKeyFile=(none)
spark.kubernetes.authenticate.driver.mounted.clientCertFile=(none)
spark.kubernetes.authenticate.driver.mounted.oauthTokenFile=(none)
spark.kubernetes.authenticate.driver.serviceAccountName=default
spark.kubernetes.authenticate.executor.serviceAccountName=(value of spark.kubernetes.authenticate.driver.serviceAccountName)
spark.kubernetes.authenticate.caCertFile=(none)
spark.kubernetes.authenticate.clientKeyFile=(none)
spark.kubernetes.authenticate.clientCertFile=(none)
spark.kubernetes.authenticate.oauthToken=(none)
spark.kubernetes.authenticate.oauthTokenFile=(none)
spark.kubernetes.driver.label.[LabelName]=(none)
spark.kubernetes.driver.annotation.[AnnotationName]=(none)
spark.kubernetes.driver.service.label.[LabelName]=(none)
spark.kubernetes.driver.service.annotation.[AnnotationName]=(none)
spark.kubernetes.executor.label.[LabelName]=(none)
spark.kubernetes.executor.annotation.[AnnotationName]=(none)
spark.kubernetes.driver.pod.name=(none)
spark.kubernetes.executor.podNamePrefix=(none)
spark.kubernetes.submission.waitAppCompletion=true
spark.kubernetes.report.interval=1s
spark.kubernetes.executor.apiPollingInterval=30s
spark.kubernetes.driver.request.cores=(none)
spark.kubernetes.driver.limit.cores=(none)
spark.kubernetes.executor.request.cores=(none)
spark.kubernetes.executor.limit.cores=(none)
spark.kubernetes.node.selector.[labelKey]=(none)
spark.kubernetes.driver.node.selector.[labelKey]=(none)
spark.kubernetes.executor.node.selector.[labelKey]=(none)
spark.kubernetes.driverEnv.[EnvironmentVariableName]=(none)
spark.kubernetes.driver.secrets.[SecretName]=(none)
spark.kubernetes.executor.secrets.[SecretName]=(none)
spark.kubernetes.driver.secretKeyRef.[EnvName]=(none)
spark.kubernetes.executor.secretKeyRef.[EnvName]=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].label.[LabelName]=(none)
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].annotation.[AnnotationName]=(none)
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.path=(none)
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.subPath=(none)
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.readOnly=false
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].options.[OptionName]=(none)
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].label.[LabelName]=(none)
spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].annotation.[AnnotationName]=(none)
spark.kubernetes.local.dirs.tmpfs=false
spark.kubernetes.memoryOverheadFactor=0.1
spark.kubernetes.pyspark.pythonVersion="3"
spark.kubernetes.kerberos.krb5.path=(none)
spark.kubernetes.kerberos.krb5.configMapName=(none)
spark.kubernetes.hadoop.configMapName=(none)
spark.kubernetes.kerberos.tokenSecret.name=(none)
spark.kubernetes.kerberos.tokenSecret.itemKey=(none)
spark.kubernetes.driver.podTemplateFile=(none)
spark.kubernetes.driver.podTemplateContainerName=(none)
spark.kubernetes.executor.podTemplateFile=(none)
spark.kubernetes.executor.podTemplateContainerName=(none)
spark.kubernetes.executor.deleteOnTermination=true
spark.kubernetes.executor.checkAllContainers=true
spark.kubernetes.submission.connectionTimeout=10000
spark.kubernetes.submission.requestTimeout=10000
spark.kubernetes.trust.certificates=false
spark.kubernetes.driver.connectionTimeout=10000
spark.kubernetes.driver.requestTimeout=10000
spark.kubernetes.appKillPodDeletionGracePeriod=(none)
spark.kubernetes.dynamicAllocation.deleteGracePeriod=5s
spark.kubernetes.file.upload.path=(none)
spark.kubernetes.executor.decommissionLabel=(none)
spark.kubernetes.executor.decommissionLabelValue=(none)
spark.kubernetes.executor.scheduler.name=(none)
spark.kubernetes.driver.scheduler.name=(none)
spark.kubernetes.scheduler.name=(none)
spark.kubernetes.configMap.maxSize=1048576
spark.kubernetes.executor.missingPodDetectDelta=30s
spark.kubernetes.decommission.script=/opt/decom.sh
spark.kubernetes.driver.service.deleteOnTermination=true
spark.kubernetes.driver.service.ipFamilyPolicy=SingleStack
spark.kubernetes.driver.service.ipFamilies=IPv4
spark.kubernetes.driver.ownPersistentVolumeClaim=true
spark.kubernetes.driver.reusePersistentVolumeClaim=true
spark.kubernetes.driver.waitToReusePersistentVolumeClaim=false
spark.kubernetes.executor.disableConfigMap=false
spark.kubernetes.driver.pod.featureSteps=(none)
spark.kubernetes.executor.pod.featureSteps=(none)
spark.kubernetes.allocation.maxPendingPods=Int.MaxValue
spark.kubernetes.allocation.pods.allocator=direct
spark.kubernetes.allocation.executor.timeout=600s
spark.kubernetes.allocation.driver.readinessTimeout=1s
spark.kubernetes.executor.enablePollingWithResourceVersion=false
spark.kubernetes.executor.eventProcessingInterval=1s
spark.kubernetes.executor.rollInterval=0s
spark.kubernetes.executor.minTasksPerExecutorBeforeRolling=0
spark.kubernetes.executor.rollPolicy=OUTLIER
spark.master.ui.port=8080
spark.master.ui.title=(None)
spark.master.ui.decommission.allow.mode=LOCAL
spark.master.ui.historyServerUrl=(None)
spark.master.rest.enabled=false
spark.master.rest.host=(None)
spark.master.rest.port=6066
spark.master.rest.filters=(None)
spark.master.useAppNameAsAppId.enabled=false
spark.deploy.retainedApplications=200
spark.deploy.retainedDrivers=200
spark.deploy.spreadOutDrivers=true
spark.deploy.spreadOutApps=true
spark.deploy.defaultCores=Int.MaxValue
spark.deploy.maxExecutorRetries=10
spark.deploy.maxDrivers=Int.MaxValue
spark.deploy.appNumberModulo=(None)
spark.deploy.driverIdPattern=driver-%s-%04d
spark.deploy.appIdPattern=app-%s-%04d
spark.worker.timeout=60
spark.dead.worker.persistence=15
spark.worker.resource.{name}.amount=(none)
spark.worker.resource.{name}.discoveryScript=(none)
spark.worker.resourcesFile=(none)
spark.worker.initialRegistrationRetries=6
spark.worker.maxRegistrationRetries=16
spark.worker.cleanup.enabled=true
spark.worker.cleanup.interval=1800 (30 minutes)
spark.worker.cleanup.appDataTtl=604800 (7 days, 7 * 24 * 3600)
spark.shuffle.service.db.enabled=true
spark.shuffle.service.db.backend=ROCKSDB
spark.storage.cleanupFilesAfterExecutorExit=true
spark.worker.ui.compressedLogFileLengthCacheSize=100
spark.worker.idPattern=worker-%s-%s-%d
spark.standalone.submit.waitAppCompletion=false
spark.deploy.recoveryMode=NONE
spark.deploy.recoveryDirectory=""
spark.deploy.recoveryCompressionCodec=(none)
spark.deploy.recoveryTimeout=(none)
spark.deploy.recoveryMode.factory=""
spark.deploy.zookeeper.url=None
spark.deploy.zookeeper.dir=None
