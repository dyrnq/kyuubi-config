[
  {
    "Key": "name",
    "Default value": "No default",
    "Description": "Unique name for the connector. Attempting to register again with the same name will fail. This property is required by all Kafka Connect connectors."
  },
  {
    "Key": "connector.class",
    "Default value": "No default",
    "Description": "The name of the Java class for the connector. Always use a value of io.debezium.connector.postgresql.PostgresConnector for the PostgreSQL connector."
  },
  {
    "Key": "tasks.max",
    "Default value": "1",
    "Description": "The maximum number of tasks that should be created for this connector. The PostgreSQL connector always uses a single task and therefore does not use this value, so the default is always acceptable."
  },
  {
    "Key": "plugin.name",
    "Default value": "decoderbufs",
    "Description": "The name of the PostgreSQL logical decoding plug-in installed on the PostgreSQL server. Supported values are decoderbufs, and pgoutput."
  },
  {
    "Key": "slot.name",
    "Default value": "debezium",
    "Description": "The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the Debezium connector that you are configuring. Slot names must conform to PostgreSQL replication slot naming rules, which state: \"Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character.\""
  },
  {
    "Key": "slot.drop.on.stop",
    "Default value": "false",
    "Description": "Whether or not to delete the logical replication slot when the connector stops in a graceful, expected way. The default behavior is that the replication slot remains configured for the connector when the connector stops. When the connector restarts, having the same replication slot enables the connector to start processing where it left off. Set to true in only testing or development environments. Dropping the slot allows the database to discard WAL segments. When the connector restarts it performs a new snapshot or it can continue from a persistent offset in the Kafka Connect offsets topic."
  },
  {
    "Key": "slot.failover",
    "Default value": "false",
    "Description": "Specifies whether the connector creates a failover slot. If you omit this setting, or if the primary server runs PostgreSQL 16 or earlier, the connector does not create a failover slot. PostgreSQL uses the synchronized_standby_slots parameter to configure replication slot synchronization between primary and standby servers. Set this parameter on the primary server to specify the physical replication slots that it synchronizes with on standby servers."
  },
  {
    "Key": "publication.name",
    "Default value": "dbz_publication",
    "Description": "The name of the PostgreSQL publication created for streaming changes when using pgoutput. This publication is created at start-up if it does not already exist and it includes all tables. Debezium then applies its own include/exclude list filtering, if configured, to limit the publication to change events for the specific tables of interest. The connector user must have superuser permissions to create this publication, so it is usually preferable to create the publication before starting the connector for the first time. If the publication already exists, either for all tables or configured with a subset of tables, Debezium uses the publication as it is defined."
  },
  {
    "Key": "database.hostname",
    "Default value": "No default",
    "Description": "IP address or hostname of the PostgreSQL database server."
  },
  {
    "Key": "database.port",
    "Default value": "5432",
    "Description": "Integer port number of the PostgreSQL database server."
  },
  {
    "Key": "database.user",
    "Default value": "No default",
    "Description": "Name of the PostgreSQL database user for connecting to the PostgreSQL database server."
  },
  {
    "Key": "database.password",
    "Default value": "No default",
    "Description": "Password to use when connecting to the PostgreSQL database server."
  },
  {
    "Key": "database.dbname",
    "Default value": "No default",
    "Description": "The name of the PostgreSQL database from which to stream the changes."
  },
  {
    "Key": "topic.prefix",
    "Default value": "No default",
    "Description": "Topic prefix that provides a namespace for the particular PostgreSQL database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used in the database server logical name. Do not change the value of this property. If you change the name value, after a restart, instead of continuing to emit events to the original topics, the connector emits subsequent events to topics whose names are based on the new value."
  },
  {
    "Key": "schema.include.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match names of schemas for which you want to capture changes. Any schema name not included in schema.include.list is excluded from having its changes captured. By default, all non-system schemas have their changes captured. To match the name of a schema, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the schema; it does not match substrings that might be present in a schema name. If you include this property in the configuration, do not also set the schema.exclude.list property."
  },
  {
    "Key": "schema.exclude.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match names of schemas for which you do not want to capture changes. Any schema whose name is not included in schema.exclude.list has its changes captured, with the exception of system schemas. To match the name of a schema, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the schema; it does not match substrings that might be present in a schema name. If you include this property in the configuration, do not set the schema.include.list property."
  },
  {
    "Key": "table.include.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form schemaName.tableName. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. If you include this property in the configuration, do not also set the table.exclude.list property."
  },
  {
    "Key": "table.exclude.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form schemaName.tableName. When this property is set, the connector captures changes from every table that you do not specify. To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. If you include this property in the configuration, do not set the table.include.list property."
  },
  {
    "Key": "column.include.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns that should be included in change event record values. Fully-qualified names for columns are of the form schemaName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the expression is used to match the entire name string of the column; it does not match substrings that might be present in a column name. If you include this property in the configuration, do not also set the column.exclude.list property."
  },
  {
    "Key": "column.exclude.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns that should be excluded from change event record values. Fully-qualified names for columns are of the form schemaName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the expression is used to match the entire name string of the column; it does not match substrings that might be present in a column name. If you include this property in the configuration, do not set the column.include.list property."
  },
  {
    "Key": "skip.messages.without.change",
    "Default value": "false",
    "Description": "Specifies whether to skip publishing messages when there is no change in included columns. This would essentially filter messages if there is no change in columns included as per column.include.list or column.exclude.list properties. This property is applied only when the REPLICA IDENTITY of the table is set to FULL."
  },
  {
    "Key": "time.precision.mode",
    "Default value": "adaptive",
    "Description": "Time, date, and timestamps can be represented with different kinds of precision: adaptive captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column’s type. adaptive_time_microseconds captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column’s type. An exception is TIME type fields, which are always captured as microseconds. connect always represents time and timestamp values by using Kafka Connect’s built-in representations for Time, Date, and Timestamp, which use millisecond precision regardless of the database columns\u0027 precision. For more information, see temporal values."
  },
  {
    "Key": "decimal.handling.mode",
    "Default value": "precise",
    "Description": "Specifies how the connector should handle values for DECIMAL and NUMERIC columns: precise represents values by using java.math.BigDecimal to represent values in binary form in change events. double represents values by using double values, which might result in a loss of precision but which is easier to use. string encodes values as formatted strings, which are easy to consume but semantic information about the real type is lost. For more information, see Decimal types."
  },
  {
    "Key": "hstore.handling.mode",
    "Default value": "json",
    "Description": "Specifies how the connector should handle values for hstore columns: map represents values by using MAP. json represents values by using json string. This setting encodes values as formatted strings such as {\"key\" : \"val\"}. For more information, see PostgreSQL HSTORE type."
  },
  {
    "Key": "interval.handling.mode",
    "Default value": "numeric",
    "Description": "Specifies how the connector should handle values for interval columns: numeric represents intervals using approximate number of microseconds. string represents intervals exactly by using the string pattern representation P\u003cyears\u003eY\u003cmonths\u003eM\u003cdays\u003eDT\u003chours\u003eH\u003cminutes\u003eM\u003cseconds\u003eS. For example: P1Y2M3DT4H5M6.78S. For more information, see PostgreSQL basic types."
  },
  {
    "Key": "database.sslmode",
    "Default value": "prefer",
    "Description": "Whether to use an encrypted connection to the PostgreSQL server. Options include: disable uses an unencrypted connection. allow attempts to use an unencrypted connection first and, failing that, a secure (encrypted) connection. prefer attempts to use a secure (encrypted) connection first and, failing that, an unencrypted connection. require uses a secure (encrypted) connection, and fails if one cannot be established. verify-ca behaves like require but also verifies the server TLS certificate against the configured Certificate Authority (CA) certificates, or fails if no valid matching CA certificates are found. verify-full behaves like verify-ca but also verifies that the server certificate matches the host to which the connector is trying to connect. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "database.sslcert",
    "Default value": "No default",
    "Description": "The path to the file that contains the SSL certificate for the client. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "database.sslkey",
    "Default value": "No default",
    "Description": "The path to the file that contains the SSL private key of the client. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "database.sslpassword",
    "Default value": "No default",
    "Description": "The password to access the client private key from the file specified by database.sslkey. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "database.sslrootcert",
    "Default value": "No default",
    "Description": "The path to the file that contains the root certificate(s) against which the server is validated. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "database.sslfactory",
    "Default value": "No default",
    "Description": "A name of the class that creates SSL Sockets. Use org.postgresql.ssl.NonValidatingFactory to disable SSL validation in development environments."
  },
  {
    "Key": "database.tcpKeepAlive",
    "Default value": "true",
    "Description": "Enable TCP keep-alive probe to verify that the database connection is still alive. For more information, see the PostgreSQL documentation."
  },
  {
    "Key": "tombstones.on.delete",
    "Default value": "true",
    "Description": "Controls whether a delete event is followed by a tombstone event. true - a delete operation is represented by a delete event and a subsequent tombstone event. false - only a delete event is emitted. After a source record is deleted, emitting a tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case log compaction is enabled for the topic."
  },
  {
    "Key": "column.truncate.to.length.chars",
    "Default value": "n/a",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want to truncate the data in a set of columns when it exceeds the number of characters specified by the length in the property name. Set length to a positive integer value, for example, column.truncate.to.20.chars. The fully-qualified name of a column observes the following format: \u003cschemaName\u003e.\u003ctableName\u003e.\u003ccolumnName\u003e. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. You can specify multiple properties with different lengths in a single configuration."
  },
  {
    "Key": "column.mask.with.length.chars",
    "Default value": "n/a",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want the connector to mask the values for a set of columns, for example, if they contain sensitive data. Set length to a positive integer to replace data in the specified columns with the number of asterisk (*) characters specified by the length in the property name. Set length to 0 (zero) to replace data in the specified columns with an empty string. The fully-qualified name of a column observes the following format: schemaName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. You can specify multiple properties with different lengths in a single configuration."
  },
  {
    "Key": "column.mask.hash.hashAlgorithm.with.salt.salt; column.mask.hash.v2.hashAlgorithm.with.salt.salt",
    "Default value": "n/a",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form \u003cschemaName\u003e.\u003ctableName\u003e.\u003ccolumnName\u003e. To match the name of a column Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. In the resulting change event record, the values for the specified columns are replaced with pseudonyms. A pseudonym consists of the hashed value that results from applying the specified hashAlgorithm and salt. Based on the hash function that is used, referential integrity is maintained, while column values are replaced with pseudonyms. Supported hash functions are described in the MessageDigest section of the Java Cryptography Architecture Standard Algorithm Name Documentation. In the following example, CzQMA0cB5K is a randomly selected salt. column.mask.hash.SHA-256.with.salt.CzQMA0cB5K \u003d inventory.orders.customerName, inventory.shipment.customerName If necessary, the pseudonym is automatically shortened to the length of the column. The connector configuration can include multiple properties that specify different hash algorithms and salts. Depending on the hashAlgorithm used, the salt selected, and the actual data set, the resulting data set might not be completely masked. Hashing strategy version 2 should be used to ensure fidelity if the value is being hashed in different places or systems."
  },
  {
    "Key": "column.propagate.source.type",
    "Default value": "n/a",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns for which you want the connector to emit extra parameters that represent column metadata. When this property is set, the connector adds the following fields to the schema of event records: __debezium.source.column.type __debezium.source.column.length __debezium.source.column.scale These parameters propagate a column’s original type name and length (for variable-width types), respectively. Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases. The fully-qualified name of a column observes one of the following formats: databaseName.tableName.columnName, or databaseName.schemaName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name."
  },
  {
    "Key": "datatype.propagate.source.type",
    "Default value": "n/a",
    "Description": "An optional, comma-separated list of regular expressions that specify the fully-qualified names of data types that are defined for columns in a database. When this property is set, for columns with matching data types, the connector emits event records that include the following extra fields in their schema: __debezium.source.column.type __debezium.source.column.length __debezium.source.column.scale These parameters propagate a column’s original type name and length (for variable-width types), respectively. Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases. The fully-qualified name of a column observes one of the following formats: databaseName.tableName.typeName, or databaseName.schemaName.tableName.typeName. To match the name of a data type, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the data type; the expression does not match substrings that might be present in a type name. For the list of PostgreSQL-specific data type names, see the PostgreSQL data type mappings."
  },
  {
    "Key": "message.key.columns",
    "Default value": "empty string",
    "Description": "A list of expressions that specify the columns that the connector uses to form custom message keys for change event records that it publishes to the Kafka topics for specified tables. By default, Debezium uses the primary key column of a table as the message key for records that it emits. In place of the default, or to specify a key for tables that lack a primary key, you can configure custom message keys based on one or more columns. To establish a custom message key for a table, list the table, followed by the columns to use as the message key. Each list entry takes the following format: \u003cfully-qualified_tableName\u003e:\u003ckeyColumn\u003e,\u003ckeyColumn\u003e To base a table key on multiple column names, insert commas between the column names. Each fully-qualified table name is a regular expression in the following format: \u003cschemaName\u003e.\u003ctableName\u003e The property can include entries for multiple tables. Use a semicolon to separate table entries in the list. The following example sets the message key for the tables inventory.customers and purchase.orders: inventory.customers:pk1,pk2;(.*).purchaseorders:pk3,pk4 In the example, the columns pk1 and pk2 are specified as the message key for the table inventory.customer. For the purchaseorders tables in any schema, the columns pk3 and pk4 serve as the message key. There is no limit to the number of columns that you use to create custom message keys. However, it’s best to use the minimum number that are required to specify a unique key. If the expressions that you specify for this property match columns that are not part of the table’s primary key, set the REPLICA IDENTITY of the table to FULL. If you set REPLICA IDENTITY to another value, such as DEFAULT, after delete operations, the connector fails to generate tombstone events with the expected null values."
  },
  {
    "Key": "publication.autocreate.mode",
    "Default value": "all_tables",
    "Description": "Specifies whether and how the connector creates a publication. This setting applies only when the connector streams changes by using the pgoutput plug-in. To create publications, the connector must access PostgreSQL through a database account that has specific permissions. For more information, see Setting privileges to enable Debezium to create PostgreSQL publications. Specify one of the following values: all_tables If a publication exists, the connector uses it. If a publication does not exist, the connector creates a publication for all tables in the database from which the connector captures changes. The connector runs the following SQL command to create a publication: CREATE PUBLICATION \u003cpublication_name\u003e FOR ALL TABLES; disabled The connector does not attempt to create a publication. A database administrator or the user configured to perform replications must have created the publication before running the connector. If the connector cannot find the publication, the connector throws an exception and stops. filtered If a publication does not exist, the connector creates one by running a SQL command in the following format: CREATE PUBLICATION \u003cpublication_name\u003e FOR TABLE \u003ctbl1, tbl2, tbl3\u003e The resulting publication includes tables that match the current filter configuration, as specified by the schema.include.list, schema.exclude.list, table.include.list, and table.exclude.list connector configuration properties. If the publication exists, the connector updates the publication for tables that match the current filter configuration by running a SQL command in the following format: ALTER PUBLICATION \u003cpublication_name\u003e SET TABLE \u003ctbl1, tbl2, tbl3\u003e. no_tables If a publication exists, the connector uses it. If a publication does not exist, the connector creates a publication without specifying any table by running a SQL command in the following format: CREATE PUBLICATION \u003cpublication_name\u003e; Set the no_tables option if you want the connector to capture only logical decoding messages, and not capture any other change events, such as those caused by INSERT, UPDATE, and DELETE operations on any table. If you select this option, to prevent the connector from emitting and processing READ events, you can specify names of schemas or tables for which you do not want to capture changes, for example, by using \"table.exclude.list\": \"public.*\" or \"schema.exclude.list\": \"public\"."
  },
  {
    "Key": "replica.identity.autoset.values",
    "Default value": "empty string",
    "Description": "Set this property to apply specific replica identity settings to a subset of the tables that a connector captures, based on the table name. The replica identity values that the property sets overwrite the replica identity values that are set in the database. The property accepts a comma-separated list of key-value pairs. Each key is a regular expression that matches fully-qualified table names; the corresponding value specifies a replica identity type. For example: \u003cfqTableNameA\u003e:\u003creplicaIdentity1\u003e,\u003cfqTableNameB\u003e:\u003creplicaIdentity2\u003e,\u003cfqTableNameC\u003e:\u003creplicaIdentity3\u003e Use the following format to specify the fully qualified table name: SchemaName.TableName Set the replica identity to one of the following values: DEFAULT Records the value, if one existed, that was set for the primary key column before the change event. This is the default setting for non-system tables. INDEX indexName Records the values that were set for all columns defined for a specified index before the change event. The index must be unique, not partial, not deferrable, and must include only columns marked NOT NULL. If the specified index is dropped, the resulting behavior is the same as if you set the value to NOTHING. FULL Records the values that were set for all columns in the row before the change event. NOTHING Records no information about the row state before the change event. This is the default value for system tables. Example: schema1.*:FULL,schema2.table2:NOTHING,schema2.table3:INDEX idx_name The replica.identity.autoset.values property applies only to tables that the connector captures. Other tables are ignored, even if they match the specified expression. Use the following connector properties to designate the tables to capture: table.include.list table.exclude.list schema.include.list schema.exclude.list"
  },
  {
    "Key": "binary.handling.mode",
    "Default value": "bytes",
    "Description": "Specifies how binary (bytea) columns should be represented in change events. Specify one of the following values: bytes Represents binary data as a byte array. base64 Represents binary data as base64-encoded strings. base64-url-safe Represents binary data as base64-url-safe-encoded strings. hex Represents binary data as hex-encoded (base16) strings."
  },
  {
    "Key": "schema.name.adjustment.mode",
    "Default value": "none",
    "Description": "Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Set one of the following values: none Does not apply any adjustment. avro Replaces the characters that cannot be used in the Avro type name with underscore. avro_unicode Replaces the underscore or characters that cannot be used in the Avro type name with corresponding Unicode characters, such as _uxxxx. In the preceding example, the underscore character (_) represents an escape sequence, equivalent to a backslash in Java."
  },
  {
    "Key": "field.name.adjustment.mode",
    "Default value": "none",
    "Description": "Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Specify one of the following values: none Do not apply any adjustment. avro Replace characters that cannot be used in Avro type names with underscores. avro_unicode Replace the underscore or characters that cannot be used in Avro type names with the corresponding Unicode characters, such as _uxxxx. In the preceding example, the underscore character (_) represents an escape sequence, equivalent to a backslash in Java. For more information, see Avro naming."
  },
  {
    "Key": "money.fraction.digits",
    "Default value": "2",
    "Description": "Specifies how many decimal digits should be used when converting Postgres money type to java.math.BigDecimal, which represents the values in change events. Applicable only when decimal.handling.mode is set to precise."
  },
  {
    "Key": "message.prefix.include.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the names of the logical decoding message prefixes that you want the connector to capture. By default, the connector captures all logical decoding messages. When this property is set, the connector captures only logical decoding message with the prefixes specified by the property. All other logical decoding messages are excluded. To match the name of a message prefix, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire message prefix string; the expression does not match substrings that might be present in a prefix. If you include this property in the configuration, do not also set the message.prefix.exclude.list property. For information about the structure of message events and about their ordering semantics, see message events."
  },
  {
    "Key": "message.prefix.exclude.list",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the names of the logical decoding message prefixes that you do not want the connector to capture. When this property is set, the connector does not capture logical decoding messages that use the specified prefixes. All other messages are captured. To exclude all logical decoding messages, set the value of this property to .*. To match the name of a message prefix, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire message prefix string; the expression does not match substrings that might be present in a prefix. If you include this property in the configuration, do not also set message.prefix.include.list property. For information about the structure of message events and about their ordering semantics, see message events."
  },
  {
    "Key": "converters",
    "Default value": "No default",
    "Description": "Enumerates a comma-separated list of the symbolic names of the custom converter instances that the connector can use. For example, isbn You must set the converters property to enable the connector to use a custom converter. For each converter that you configure for a connector, you must also add a .type property, which specifies the fully-qualified name of the class that implements the converter interface. The .type property uses the following format: \u003cconverterSymbolicName\u003e.type For example, isbn.type: io.debezium.test.IsbnConverter If you want to further control the behavior of a configured converter, you can add one or more configuration parameters to pass values to the converter. To associate any additional configuration parameter with a converter, prefix the parameter names with the symbolic name of the converter. For example, isbn.schema.name: io.debezium.postgresql.type.Isbn"
  },
  {
    "Key": "snapshot.isolation.mode",
    "Default value": "serializable",
    "Description": "Specifies the transaction isolation level and the type of locking, if any, that the connector applies when it reads data during an initial snapshot or ad hoc blocking snapshot. Each isolation level strikes a different balance between optimizing concurrency and performance on the one hand, and maximizing data consistency and accuracy on the other. Snapshots that use stricter isolation levels result in higher quality, more consistent data, but the cost of the improvement is decreased performance due to longer lock times and fewer concurrent transactions. Less restrictive isolation levels can increase efficiency, but at the expense of inconsistent data. For more information about transaction isolation levels in PostgreSQL, see the PostgreSQL documentation. Specify one of the following isolation levels: serializable The default, and most restrictive isolation level. This option prevents serialization anomalies and provides the highest degree of data integrity. To ensure the data consistency of captured tables, a snapshot runs in a transaction that uses a repeatable read isolation level, blocking concurrent DDL changes on the tables, and locking the database to index creation. When this option is set, users or administrators cannot perform certain operations, such as creating a table index, until the snapshot concludes. The entire range of table keys remains locked until the snapshot completes. This option matches the snapshot behavior that was available in the connector before the introduction of this property. repeatable_read Prevents other transactions from updating table rows during the snapshot. New records captured by the snapshot can appear twice; first, as part of the initial snapshot, and then again in the streaming phase. However, this level of consistency is tolerable for database mirroring. Ensures data consistency between the tables being scanned and blocking DDL on the selected tables, and concurrent index creation throughout the database. Allows for serialization anomalies. read_committed In PostgreSQL, there is no difference between the behavior of the Read Uncommitted and Read Committed isolation modes. As a result, for this property, the read_committed option effectively provides the least restrictive level of isolation. Setting this option sacrifices some consistency for initial and ad hoc blocking snapshots, but provides better database performance for other users during the snapshot. In general, this transaction consistency level is appropriate for data mirroring. Other transactions cannot update table rows during the snapshot. However, minor data inconsistencies can occur when a record is added during the initial snapshot, and the connector later recaptures the record after the streaming phase begins. read_uncommitted Nominally, this option offers the least restrictive level of isolation. However, as explained in the description for the read-committed option, for the Debezium PostgreSQL connector, this option provides the same level of isolation as the read_committed option."
  },
  {
    "Key": "snapshot.mode",
    "Default value": "initial",
    "Description": "Specifies the criteria for performing a snapshot when the connector starts: always The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts. After the snapshot completes, the connector begins to stream event records for subsequent database changes. initial The connector performs a snapshot only when no offsets have been recorded for the logical server name. initial_only The connector performs an initial snapshot and then stops, without processing any subsequent changes. no_data The connector never performs snapshots. When a connector is configured this way, after it starts, it behaves as follows: If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN is stored, the connector starts streaming changes from the point in time when the PostgreSQL logical replication slot was created on the server. Use this snapshot mode only when you know all data of interest is still reflected in the WAL. never Deprecated see no_data. when_needed After the connector starts, it performs a snapshot only if it detects one of the following circumstances: It cannot detect any topic offsets. A previously recorded offset specifies a log position that is not available on the server. configuration_based With this option, you control snapshot behavior through a set of connector properties that have the prefix \u0027snapshot.mode.configuration.based\u0027. custom The connector performs a snapshot according to the implementation specified by the snapshot.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.Snapshotter interface. For more information, see the table of snapshot.mode options."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.data",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes table data when it performs a snapshot."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.schema",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes the table schema when it performs a snapshot."
  },
  {
    "Key": "snapshot.mode.configuration.based.start.stream",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector begins to stream change events after a snapshot completes."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.on.schema.error",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes table schema in a snapshot if the schema history topic is not available."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.on.data.error",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, this property specifies whether the connector attempts to snapshot table data if it does not find the last committed offset in the transaction log. Set the value to true to instruct the connector to perform a new snapshot."
  },
  {
    "Key": "snapshot.mode.custom.name",
    "Default value": "No default",
    "Description": "When snapshot.mode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the \u0027io.debezium.spi.snapshot.Snapshotter\u0027 interface. The provided implementation is called after a connector restart to determine whether to perform a snapshot. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.locking.mode",
    "Default value": "none",
    "Description": "Specifies how the connector holds locks on tables while performing a schema snapshot. Set one of the following options: shared The connector holds a table lock that prevents exclusive table access during the initial portion phase of the snapshot in which database schemas and other metadata are read. After the initial phase, the snapshot no longer requires table locks. none The connector avoids locks entirely. Do not use this mode if schema changes might occur during the snapshot. custom The connector performs a snapshot according to the implementation specified by the snapshot.locking.mode.custom.name property, which is a custom implementation of the io.debezium.spi.snapshot.SnapshotLock interface."
  },
  {
    "Key": "snapshot.locking.mode.custom.name",
    "Default value": "No default",
    "Description": "When snapshot.locking.mode is set to custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the \u0027io.debezium.spi.snapshot.SnapshotLock\u0027 interface. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.query.mode",
    "Default value": "select_all",
    "Description": "Specifies how the connector queries data while performing a snapshot. Set one of the following options: select_all The connector performs a select all query by default, optionally adjusting the columns selected based on the column include and exclude list configurations. custom The connector performs a snapshot query according to the implementation specified by the snapshot.query.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.SnapshotQuery interface. This setting enables you to manage snapshot content in a more flexible manner compared to using the snapshot.select.statement.overrides property."
  },
  {
    "Key": "snapshot.query.mode.custom.name",
    "Default value": "No default",
    "Description": "When snapshot.query.mode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the \u0027io.debezium.spi.snapshot.SnapshotQuery\u0027 interface. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.include.collection.list",
    "Default value": "All tables specified in table.include.list",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names (\u003cschemaName\u003e.\u003ctableName\u003e) of the tables to include in a snapshot. The specified items must be named in the connector’s table.include.list property. This property takes effect only if the connector’s snapshot.mode property is set to a value other than never. This property does not affect the behavior of incremental snapshots. To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name."
  },
  {
    "Key": "snapshot.lock.timeout.ms",
    "Default value": "10000",
    "Description": "Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If the connector cannot acquire table locks in this time interval, the snapshot fails. How the connector performs snapshots provides details."
  },
  {
    "Key": "snapshot.select.statement.overrides",
    "Default value": "No default",
    "Description": "Specifies the table rows to include in a snapshot. Use the property if you want a snapshot to include only a subset of the rows in a table. This property affects snapshots only. It does not apply to events that the connector reads from the log. The property contains a comma-separated list of fully-qualified table names in the form \u003cschemaName\u003e.\u003ctableName\u003e. For example, \"snapshot.select.statement.overrides\": \"inventory.products,customers.orders\" For each table in the list, add a further configuration property that specifies the SELECT statement for the connector to run on the table when it takes a snapshot. The specified SELECT statement determines the subset of table rows to include in the snapshot. Use the following format to specify the name of this SELECT statement property: snapshot.select.statement.overrides.\u003cschemaName\u003e.\u003ctableName\u003e. For example, snapshot.select.statement.overrides.customers.orders. Example: From a customers.orders table that includes the soft-delete column, delete_flag, add the following properties if you want a snapshot to include only those records that are not soft-deleted: \"snapshot.select.statement.overrides\": \"customer.orders\",\n\"snapshot.select.statement.overrides.customer.orders\": \"SELECT * FROM customers.orders WHERE delete_flag \u003d 0 ORDER BY id DESC\" In the resulting snapshot, the connector includes only the records for which delete_flag \u003d 0."
  },
  {
    "Key": "event.processing.failure.handling.mode",
    "Default value": "fail",
    "Description": "Specifies how the connector should react to exceptions during processing of events: fail propagates the exception, indicates the offset of the problematic event, and causes the connector to stop. warn logs the offset of the problematic event, skips that event, and continues processing. skip skips the problematic event and continues processing."
  },
  {
    "Key": "max.batch.size",
    "Default value": "2048",
    "Description": "Positive integer value that specifies the maximum size of each batch of events that the connector processes."
  },
  {
    "Key": "max.queue.size",
    "Default value": "8192",
    "Description": "Positive integer value that specifies the maximum number of records that the blocking queue can hold. When Debezium reads events streamed from the database, it places the events in the blocking queue before it writes them to Kafka. The blocking queue can provide backpressure for reading change events from the database in cases where the connector ingests messages faster than it can write them to Kafka, or when Kafka becomes unavailable. Events that are held in the queue are disregarded when the connector periodically records offsets. Always set the value of max.queue.size to be larger than the value of max.batch.size."
  },
  {
    "Key": "max.queue.size.in.bytes",
    "Default value": "0",
    "Description": "A long integer value that specifies the maximum volume of the blocking queue in bytes. By default, volume limits are not specified for the blocking queue. To specify the number of bytes that the queue can consume, set this property to a positive long value. If max.queue.size is also set, writing to the queue is blocked when the size of the queue reaches the limit specified by either property. For example, if you set max.queue.size\u003d1000, and max.queue.size.in.bytes\u003d5000, writing to the queue is blocked after the queue contains 1000 records, or after the volume of the records in the queue reaches 5000 bytes."
  },
  {
    "Key": "poll.interval.ms",
    "Default value": "500",
    "Description": "Positive integer value that specifies the number of milliseconds the connector should wait for new change events to appear before it starts processing a batch of events. Defaults to 500 milliseconds."
  },
  {
    "Key": "include.unknown.datatypes",
    "Default value": "false",
    "Description": "Specifies connector behavior when the connector encounters a field whose data type is unknown. The default behavior is that the connector omits the field from the change event and logs a warning. Set this property to true if you want the change event to contain an opaque binary representation of the field. This lets consumers decode the field. You can control the exact representation by setting the binary handling mode property. Consumers risk backward compatibility issues when include.unknown.datatypes is set to true. Not only may the database-specific binary representation change between releases, but if the data type is eventually supported by Debezium, the data type will be sent downstream in a logical type, which would require adjustments by consumers. In general, when encountering unsupported data types, create a feature request so that support can be added."
  },
  {
    "Key": "database.initial.statements",
    "Default value": "No default",
    "Description": "A semicolon separated list of SQL statements that the connector executes when it establishes a JDBC connection to the database. To use a semicolon as a character and not as a delimiter, specify two consecutive semicolons, ;;. The connector may establish JDBC connections at its own discretion. Consequently, this property is useful for configuration of session parameters only, and not for executing DML statements. The connector does not execute these statements when it creates a connection for reading the transaction log."
  },
  {
    "Key": "status.update.interval.ms",
    "Default value": "10000",
    "Description": "Frequency for sending replication connection status updates to the server, given in milliseconds. The property also controls how frequently the database status is checked to detect a dead connection in case the database was shut down."
  },
  {
    "Key": "heartbeat.interval.ms",
    "Default value": "0",
    "Description": "Controls how frequently the connector sends heartbeat messages to a Kafka topic. The default behavior is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages. Heartbeat messages are needed when there are many updates in a database that is being tracked but only a tiny number of updates are related to the table(s) and schema(s) for which the connector is capturing changes. In this situation, the connector reads from the database transaction log as usual but rarely emits change records to Kafka. This means that no offset updates are committed to Kafka and the connector does not have an opportunity to send the latest retrieved LSN to the database. The database retains WAL files that contain events that have already been processed by the connector. Sending heartbeat messages enables the connector to send the latest retrieved LSN to the database, which allows the database to reclaim disk space being used by no longer needed WAL files."
  },
  {
    "Key": "heartbeat.action.query",
    "Default value": "No default",
    "Description": "Specifies a query that the connector executes on the source database when the connector sends a heartbeat message. This is useful for resolving the situation described in WAL disk space consumption, where capturing changes from a low-traffic database on the same host as a high-traffic database prevents Debezium from processing WAL records and thus acknowledging WAL positions with the database. To address this situation, create a heartbeat table in the low-traffic database, and set this property to a statement that inserts records into that table, for example: INSERT INTO test_heartbeat_table (text) VALUES (\u0027test_heartbeat\u0027) This allows the connector to receive changes from the low-traffic database and acknowledge their LSNs, which prevents unbounded WAL growth on the database host."
  },
  {
    "Key": "schema.refresh.mode",
    "Default value": "columns_diff",
    "Description": "Specify the conditions that trigger a refresh of the in-memory schema for a table. columns_diff is the safest mode. It ensures that the in-memory schema stays in sync with the database table’s schema at all times. columns_diff_exclude_unchanged_toast instructs the connector to refresh the in-memory schema cache if there is a discrepancy with the schema derived from the incoming message, unless unchanged TOASTable data fully accounts for the discrepancy. This setting can significantly improve connector performance if there are frequently-updated tables that have TOASTed data that are rarely part of updates. However, it is possible for the in-memory schema to become outdated if TOASTable columns are dropped from the table."
  },
  {
    "Key": "snapshot.delay.ms",
    "Default value": "No default",
    "Description": "An interval in milliseconds that the connector should wait before performing a snapshot when the connector starts. If you are starting multiple connectors in a cluster, this property is useful for avoiding snapshot interruptions, which might cause re-balancing of connectors."
  },
  {
    "Key": "streaming.delay.ms",
    "Default value": "0",
    "Description": "Specifies the time, in milliseconds, that the connector delays the start of the streaming process after it completes a snapshot. Setting a delay interval helps to prevent the connector from restarting snapshots in the event that a failure occurs immediately after the snapshot completes, but before the streaming process begins. Set a delay value that is higher than the value of the offset.flush.interval.ms property that is set for the Kafka Connect worker."
  },
  {
    "Key": "snapshot.fetch.size",
    "Default value": "10240",
    "Description": "During a snapshot, the connector reads table content in batches of rows. This property specifies the maximum number of rows in a batch."
  },
  {
    "Key": "slot.stream.params",
    "Default value": "No default",
    "Description": "Semicolon separated list of parameters to pass to the configured logical decoding plug-in. For example, add-tables\u003dpublic.table,public.table2;include-lsn\u003dtrue."
  },
  {
    "Key": "slot.max.retries",
    "Default value": "6",
    "Description": "If connecting to a replication slot fails, this is the maximum number of consecutive attempts to connect."
  },
  {
    "Key": "slot.retry.delay.ms",
    "Default value": "10000 (10 seconds)",
    "Description": "The number of milliseconds to wait between retry attempts when the connector fails to connect to a replication slot."
  },
  {
    "Key": "unavailable.value.placeholder",
    "Default value": "__debezium_unavailable_value",
    "Description": "Specifies the constant that the connector provides to indicate that the original value is a toasted value that is not provided by the database. If the setting of unavailable.value.placeholder starts with the hex: prefix it is expected that the rest of the string represents hexadecimally encoded octets. For more information, see toasted values."
  },
  {
    "Key": "provide.transaction.metadata",
    "Default value": "false",
    "Description": "Determines whether the connector generates events with transaction boundaries and enriches change event envelopes with transaction metadata. Specify true if you want the connector to do this. For more information, see Transaction metadata."
  },
  {
    "Key": "publish.via.partition.root",
    "Default value": "false",
    "Description": "Specifies how the connector captures and emits events for changes that it captures from partitioned tables. This setting applies only if the publication.autocreate.mode property is set to all_tables or filtered, and Debezium creates the publication for the captured tables. Set one of the following options: true The connector emits change events for all partitions to a topic with the name of the base table. When the connector creates a publication, it submits a CREATE PUBLICATION statement in which the publish_via_partition_root parameter is set to true. As a result, the publication ignores the partition in which changes originate, and only records the name of the name of the source table. false The connector emits changes from each source partition to a topic that reflects the name of the partition. When the connector creates the publication, the CREATE PUBLICATION statement omits the publish_via_partition_root parameter so that the publication always uses the name of the source partition to publish change events."
  },
  {
    "Key": "flush.lsn.source",
    "Default value": "true",
    "Description": "Determines whether the connector should commit the LSN of the processed records in the source PostgreSQL database so that the WAL logs can be deleted. Specify false if you do not want the connector to commit the LSN of processed records. If you set the value of this property to false, Debezium does not acknowledge the LSN. Failure to acknowledge the LSN can lead to uncontrolled growth of the WAL logs, which stresses storage capacity, and could result degraded performance, and even data loss. To maintain normal service, if you set this property to false, you must configure some other mechanism to commit the LSN."
  },
  {
    "Key": "retriable.restart.connector.wait.ms",
    "Default value": "10000 (10 seconds)",
    "Description": "The number of milliseconds to wait before restarting a connector after a retriable error occurs."
  },
  {
    "Key": "skipped.operations",
    "Default value": "t",
    "Description": "A comma-separated list of the operation types that you want the connector to skip during streaming. You can configure the connector to skip the following types of operations: c (insert/create) u (update) d (delete) t (truncate) Set the value to none if you do not want the connector to skip any operations."
  },
  {
    "Key": "signal.data.collection",
    "Default value": "No default value",
    "Description": "Fully-qualified name of the data collection that is used to send signals to the connector. Use the following format to specify the collection name: \u003cschemaName\u003e.\u003ctableName\u003e"
  },
  {
    "Key": "signal.enabled.channels",
    "Default value": "source",
    "Description": "List of the signaling channel names that are enabled for the connector. By default, the following channels are available: source kafka file jmx Optionally, you can also implement a custom signaling channel."
  },
  {
    "Key": "notification.enabled.channels",
    "Default value": "No default",
    "Description": "List of notification channel names that are enabled for the connector. By default, the following channels are available: sink log jmx Optionally, you can also implement a custom notification channel."
  },
  {
    "Key": "incremental.snapshot.chunk.size",
    "Default value": "1024",
    "Description": "The maximum number of rows that the connector fetches and reads into memory during an incremental snapshot chunk. Increasing the chunk size provides greater efficiency, because the snapshot runs fewer snapshot queries of a greater size. However, larger chunk sizes also require more memory to buffer the snapshot data. Adjust the chunk size to a value that provides the best performance in your environment."
  },
  {
    "Key": "incremental.snapshot.watermarking.strategy",
    "Default value": "insert_insert",
    "Description": "Specifies the watermarking mechanism that the connector uses during an incremental snapshot to deduplicate events that might be captured by an incremental snapshot and then recaptured after streaming resumes. You can specify one of the following options: insert_insert When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads during the snapshot, it writes an entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, Debezium inserts a second entry to record the closing of the window. insert_delete When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads, it writes a single entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, this entry is removed. No entry is created for the signal to close the snapshot window. Set this option to prevent rapid growth of the signaling data collection."
  },
  {
    "Key": "read.only",
    "Default value": "false",
    "Description": "Specifies whether a connector writes watermarks to the signal data collection to track the progress of an incremental snapshot. Set the value to true to enable a connector that has a read-only connection to the database to use an incremental snapshot watermarking strategy that does not require writing to the signal data collection."
  },
  {
    "Key": "xmin.fetch.interval.ms",
    "Default value": "0",
    "Description": "How often, in milliseconds, the XMIN will be read from the replication slot. The XMIN value provides the lower bounds of where a new replication slot could start from. The default value of 0 disables tracking XMIN tracking."
  },
  {
    "Key": "topic.naming.strategy",
    "Default value": "io.debezium.schema.SchemaTopicNamingStrategy",
    "Description": "The name of the TopicNamingStrategy class that should be used to determine the topic name for data change, schema change, transaction, heartbeat event etc., defaults to SchemaTopicNamingStrategy."
  },
  {
    "Key": "topic.delimiter",
    "Default value": ".",
    "Description": "Specify the delimiter for topic name, defaults to .."
  },
  {
    "Key": "topic.cache.size",
    "Default value": "10000",
    "Description": "The size used for holding the topic names in bounded concurrent hash map. This cache will help to determine the topic name corresponding to a given data collection."
  },
  {
    "Key": "topic.heartbeat.prefix",
    "Default value": "__debezium-heartbeat",
    "Description": "Controls the name of the topic to which the connector sends heartbeat messages. The topic name has this pattern: topic.heartbeat.prefix.topic.prefix For example, if the topic prefix is fulfillment, the default topic name is __debezium-heartbeat.fulfillment."
  },
  {
    "Key": "topic.transaction",
    "Default value": "transaction",
    "Description": "Controls the name of the topic to which the connector sends transaction metadata messages. The topic name has this pattern: topic.prefix.topic.transaction For example, if the topic prefix is fulfillment, the default topic name is fulfillment.transaction."
  },
  {
    "Key": "snapshot.max.threads",
    "Default value": "1",
    "Description": "Specifies the number of threads that the connector uses when performing an initial snapshot. To enable parallel initial snapshots, set the property to a value greater than 1. In a parallel initial snapshot, the connector processes multiple tables concurrently. When you enable parallel initial snapshots, the threads that perform each table snapshot can require varying times to complete their work. If a snapshot for one table requires significantly more time to complete than the snapshots for other tables, threads that have completed their work sit idle. In some environments, a network device such as a load balancer or firewall, terminates connections that remain idle for an extended interval. After the snapshot completes, the connector is unable to close the connection, resulting in an exception, and an incomplete snapshot, even in cases where the connector successfully transmitted all snapshot data. If you experience this problem, revert the value of snapshot.max.threads to 1, and retry the snapshot."
  },
  {
    "Key": "custom.metric.tags",
    "Default value": "No default",
    "Description": "Defines tags that customize MBean object names by adding metadata that provides contextual information. Specify a comma-separated list of key-value pairs. Each key represents a tag for the MBean object name, and the corresponding value represents a value for the key, for example, k1\u003dv1,k2\u003dv2 The connector appends the specified tags to the base MBean object name. Tags can help you to organize and categorize metrics data. You can define tags to identify particular application instances, environments, regions, versions, and so forth. For more information, see Customized MBean names."
  },
  {
    "Key": "errors.max.retries",
    "Default value": "-1",
    "Description": "Specifies how the connector responds after an operation that results in a retriable error, such as a connection error. Set one of the following options: -1 No limit. The connector always restarts automatically, and retries the operation, regardless of the number of previous failures. 0 Disabled. The connector fails immediately, and never retries the operation. User intervention is required to restart the connector. \u003e 0 The connector restarts automatically until it reaches the specified maximum number of retries. After the next failure, the connector stops, and user intervention is required to restart it."
  },
  {
    "Key": "database.query.timeout.ms",
    "Default value": "600000 (10 minutes)",
    "Description": "Specifies the time, in milliseconds, that the connector waits for a query to complete. Set the value to 0 (zero) to remove the timeout limit."
  },
  {
    "Key": "extended.headers.enabled",
    "Default value": "true",
    "Description": "This property specifies whether Debezium adds context headers with the prefix __debezium.context. to the messages that it emits. These headers are required by the OpenLineage integration and provide metadata that enables downstream processing systems to track and identify the sources of change events. The property adds following headers: __debezium.context.connectorLogicalName The logical name of the Debezium connector. __debezium.context.taskId The unique identifier of the connector task. __debezium.context.connectorName The name of the Debezium connector."
  },
  {
    "Key": "signal.kafka.topic",
    "Default value": "\u003ctopic.prefix\u003e-signal",
    "Description": "The name of the Kafka topic that the connector monitors for ad hoc signals. If automatic topic creation is disabled, you must manually create the required signaling topic. A signaling topic is required to preserve signal ordering. The signaling topic must have a single partition."
  },
  {
    "Key": "signal.kafka.groupId",
    "Default value": "kafka-signal",
    "Description": "The name of the group ID that is used by Kafka consumers."
  },
  {
    "Key": "signal.kafka.bootstrap.servers",
    "Default value": "No default",
    "Description": "A list of the host and port pairs that the connector uses to establish its initial connection to the Kafka cluster. Each pair references the Kafka cluster that is used by the Debezium Kafka Connect process."
  },
  {
    "Key": "signal.kafka.poll.timeout.ms",
    "Default value": "100",
    "Description": "An integer value that specifies the maximum number of milliseconds that the connector waits when polling signals."
  },
  {
    "Key": "notification.sink.topic.name",
    "Default value": "No default",
    "Description": "The name of the topic that receives notifications from Debezium. This property is required when you configure the notification.enabled.channels property to include sink as one of the enabled notification channels."
  }
]
