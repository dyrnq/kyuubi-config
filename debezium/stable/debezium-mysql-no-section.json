[
  {
    "Key": "bigint.unsigned.handling.mode",
    "Default value": "long",
    "Description": "Specifies how the connector represents BIGINT UNSIGNED columns in change events. Set one of the following options: long Uses Java long data types to represent BIGINT UNSIGNED column values. Although the long type does not offer the greatest precision, it is easy implement in most consumers. In most environments, this is the preferred setting. precise Uses java.math.BigDecimal data types to represent values. The connector uses the Kafka Connect org.apache.kafka.connect.data.Decimal data type to represent values in encoded binary format. Set this option if the connector typically works with values larger than 2^63. The long data type cannot convey values of that size.",
    "long": "Uses Java long data types to represent BIGINT UNSIGNED column values. Although the long type does not offer the greatest precision, it is easy implement in most consumers. In most environments, this is the preferred setting.",
    "precise": "Uses java.math.BigDecimal data types to represent values. The connector uses the Kafka Connect org.apache.kafka.connect.data.Decimal data type to represent values in encoded binary format. Set this option if the connector typically works with values larger than 2^63. The long data type cannot convey values of that size."
  },
  {
    "Key": "binary.handling.mode",
    "Default value": "bytes",
    "Description": "Specifies how the connector represents values for binary columns, such as, blob, binary, varbinary, in change events. Set one of the following options: bytes Represents binary data as a byte array. base64 Represents binary data as a base64-encoded String. base64-url-safe Represents binary data as a base64-url-safe-encoded String. hex Represents binary data as a hex-encoded (base16) String.",
    "bytes": "Represents binary data as a byte array.",
    "base64": "Represents binary data as a base64-encoded String.",
    "base64-url-safe": "Represents binary data as a base64-url-safe-encoded String.",
    "hex": "Represents binary data as a hex-encoded (base16) String."
  },
  {
    "Key": "column.exclude.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Other columns in the source record are captured as usual. Fully-qualified names for columns are of the form databaseName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name. If you include this property in the configuration, do not also set the column.include.list property."
  },
  {
    "Key": "column.include.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to include in change event record values. Other columns are omitted from the event record. Fully-qualified names for columns are of the form databaseName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name. If you include this property in the configuration, do not set the column.exclude.list property."
  },
  {
    "Key": "column.mask.hash.hashAlgorithm.with.salt.salt"
  },
  {
    "Key": "column.mask.hash.v2.hashAlgorithm.with.salt.salt",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form \u003cdatabaseName\u003e.\u003ctableName\u003e.\u003ccolumnName\u003e. To match the name of a column Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. In the resulting change event record, the values for the specified columns are replaced with pseudonyms. A pseudonym consists of the hashed value that results from applying the specified hashAlgorithm and salt. Based on the hash function that is used, referential integrity is maintained, while column values are replaced with pseudonyms. Supported hash functions are described in the MessageDigest section of the Java Cryptography Architecture Standard Algorithm Name Documentation. In the following example, CzQMA0cB5K is a randomly selected salt. column.mask.hash.SHA-256.with.salt.CzQMA0cB5K \u003d inventory.orders.customerName, inventory.shipment.customerName If necessary, the pseudonym is automatically shortened to the length of the column. The connector configuration can include multiple properties that specify different hash algorithms and salts. Depending on the hashAlgorithm used, the salt selected, and the actual data set, the resulting data set might not be completely masked. Hashing strategy version 2 ensures fidelity of values that are hashed in different places or systems."
  },
  {
    "Key": "column.mask.with.length.chars",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want the connector to mask the values for a set of columns, for example, if they contain sensitive data. Set length to a positive integer to replace data in the specified columns with the number of asterisk (*) characters specified by the length in the property name. Set length to 0 (zero) to replace data in the specified columns with an empty string. The fully-qualified name of a column observes the following format: databaseName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. You can specify multiple properties with different lengths in a single configuration."
  },
  {
    "Key": "column.propagate.source.type",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns for which you want the connector to emit extra parameters that represent column metadata. When this property is set, the connector adds the following fields to the schema of event records: __debezium.source.column.type __debezium.source.column.length __debezium.source.column.scale These parameters propagate a column’s original type name and length (for variable-width types), respectively. Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases. The fully-qualified name of a column observes one of the following formats: databaseName.tableName.columnName, or databaseName.schemaName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name."
  },
  {
    "Key": "column.truncate.to.length.chars",
    "Default value": "No default",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want to truncate the data in a set of columns when it exceeds the number of characters specified by the length in the property name. Set length to a positive integer value, for example, column.truncate.to.20.chars. The fully-qualified name of a column observes the following format: databaseName.tableName.columnName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. You can specify multiple properties with different lengths in a single configuration."
  },
  {
    "Key": "connect.timeout.ms",
    "Default value": "30000 (30 seconds)",
    "Description": "A positive integer value that specifies the maximum time in milliseconds that the connector waits to establish a connection to the MySQL database server before the connection request times out."
  },
  {
    "Key": "connector.class",
    "Default value": "No default",
    "Description": "The name of the Java class for the connector. Always specify io.debezium.connector.mysql.MySqlConnector for the MySQL connector."
  },
  {
    "Key": "database.exclude.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match the names of databases from which you do not want the connector to capture changes. The connector captures changes in any database that is not named in the database.exclude.list. To match the name of a database, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the database; it does not match substrings that might be present in a database name. If you include this property in the configuration, do not also set the database.include.list property."
  },
  {
    "Key": "database.hostname",
    "Default value": "No default",
    "Description": "The IP address or hostname of the MySQL database server."
  },
  {
    "Key": "database.include.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match the names of the databases from which the connector captures changes. The connector does not capture changes in any database whose name is not in database.include.list. By default, the connector captures changes in all databases. To match the name of a database, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the database; it does not match substrings that might be present in a database name. If you include this property in the configuration, do not also set the database.exclude.list property."
  },
  {
    "Key": "database.jdbc.driver",
    "Default value": "com.mysql.cj.jdbc.Driver",
    "Description": "Specifies the name of the driver class that the connector uses. Set this property to configure a driver other than the one that is packaged with the connector."
  },
  {
    "Key": "database.password",
    "Default value": "No default",
    "Description": "The password of the MySQL user that the connector uses to connect to the MySQL database server."
  },
  {
    "Key": "database.port",
    "Default value": "3306",
    "Description": "Integer port number of the MySQL database server."
  },
  {
    "Key": "database.protocol",
    "Default value": "jdbc:mysql",
    "Description": "Specifies the JDBC protocol that the driver connection string uses to connect to the database."
  },
  {
    "Key": "database.server.id",
    "Default value": "No default",
    "Description": "The numeric ID of this database client. The specified ID must be unique across all currently running database processes in the MySQL cluster. To enable reading from the binlog, the connector uses this unique ID to join the MySQL database cluster as another server."
  },
  {
    "Key": "database.user",
    "Default value": "No default",
    "Description": "The name of the MySQL user that the connector uses to connect to the MySQL database server."
  },
  {
    "Key": "decimal.handling.mode",
    "Default value": "precise",
    "Description": "Specifies how the connector handles values for DECIMAL and NUMERIC columns in change events. Set one of the following options:",
    "precise": "Uses java.math.BigDecimal values in binary form to represent values precisely.",
    "double": "Uses the double data type to represent values. This option can result in a loss of precision, but it is easier for most consumers to use.",
    "string": "Encodes values as formatted strings. This option is easy to consume, but can result in the loss of semantic information about the real type."
  },
  {
    "Key": "event.deserialization.failure.handling.mode Deprecated",
    "Default value": "fail",
    "Description": "Specifies how the connector reacts after an exception occurs during deserialization of binlog events. This option is deprecated. Use the event.processing.failure.handling.mode property instead. This property accepts the following options: fail Propagates the exception, which indicates the problematic event and its binlog offset, and causes the connector to stop. warn Logs the problematic event and its binlog offset and then skips the event. ignore Passes over the problematic event and does not log anything.",
    "fail": "Propagates the exception, which indicates the problematic event and its binlog offset, and causes the connector to stop.",
    "warn": "Logs the problematic event and its binlog offset and then skips the event.",
    "ignore": "Passes over the problematic event and does not log anything."
  },
  {
    "Key": "field.name.adjustment.mode",
    "Default value": "No default",
    "Description": "Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Set one of the following options: none No adjustment. avro Replaces characters that are not valid in Avro names with underscore characters. avro_unicode Replaces underscore characters or characters that cannot be used in Avro names with corresponding unicode, such as _uxxxx. The underscore character (_) represents an escape sequence, similar to a backslash in Java For more information, see: Avro naming.",
    "none": "No adjustment.",
    "avro": "Replaces characters that are not valid in Avro names with underscore characters.",
    "avro_unicode": "Replaces underscore characters or characters that cannot be used in Avro names with corresponding unicode, such as _uxxxx. The underscore character (_) represents an escape sequence, similar to a backslash in Java For more information, see: Avro naming."
  },
  {
    "Key": "gtid.source.excludes",
    "Default value": "No default",
    "Description": "A comma-separated list of regular expressions that match source domain IDs in the GTID set that the connector uses to find the binlog position on the MySQL server. When this property is set, the connector uses only the GTID ranges that have source UUIDs that do not match any of the specified exclude patterns. To match the value of a GTID, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the GTID’s domain identifier. If you set this property, do not also set the gtid.source.includes property."
  },
  {
    "Key": "gtid.source.includes",
    "Default value": "No default",
    "Description": "A comma-separated list of regular expressions that match source domain IDs in the GTID set used that the connector uses to find the binlog position on the MySQL server. When this property is set, the connector uses only the GTID ranges that have source UUIDs that match one of the specified include patterns. To match the value of a GTID, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the GTID’s domain identifier. If you set this property, do not also set the gtid.source.excludes property."
  },
  {
    "Key": "include.query",
    "Default value": "false",
    "Description": "Boolean value that specifies whether the change event that the connector emits includes the SQL query that generated the change. Setting this property to true might expose information about tables or fields that you explicitly excluded or masked via other settings. To enable this property, the database property binlog_annotate_row_events must be set to ON. Setting this property has no effect on events that the snapshot process generates. Snapshot events do not include the original SQL query. For more information about configuring the database to return the original SQL statement for each log event, see Enabling query log events."
  },
  {
    "Key": "include.schema.changes",
    "Default value": "true",
    "Description": "Boolean value that specifies whether the connector publishes changes in the database schema to a Kafka topic with the same name as the topic prefix. The connector records each schema change with a key that contains the database name, and a value that is a JSON structure that describes the schema update. This mechanism for recording schema changes is independent of the connector’s internal recording of changes to the database schema history."
  },
  {
    "Key": "include.schema.comments",
    "Default value": "false",
    "Description": "Boolean value that specifies whether the connector parses and publishes table and column comments on metadata objects. When you set this option to true, the schema comments that the connector includes can add a significant amount of string data to each schema object. Increasing the number and size of logical schema objects increases the amount of memory that the connector uses."
  },
  {
    "Key": "inconsistent.schema.handling.mode",
    "Default value": "fail",
    "Description": "Specifies how the connector responds to binlog events that refer to tables that are not present in the internal schema representation. That is, the internal representation is not consistent with the database. Set one of the following options: fail The connector throws an exception that reports the problematic event and its binlog offset. The connector then stops. warn The connector logs the problematic event and its binlog offset, and then skips the event. skip The connector skips the problematic event and does not report it in the log.",
    "fail": "The connector throws an exception that reports the problematic event and its binlog offset. The connector then stops.",
    "warn": "The connector logs the problematic event and its binlog offset, and then skips the event.",
    "skip": "The connector skips the problematic event and does not report it in the log."
  },
  {
    "Key": "message.key.columns",
    "Default value": "No default",
    "Description": "A list of expressions that specify the columns that the connector uses to form custom message keys for change event records that it publishes to the Kafka topics for specified tables. By default, Debezium uses the primary key column of a table as the message key for records that it emits. In place of the default, or to specify a key for tables that lack a primary key, you can configure custom message keys based on one or more columns. To establish a custom message key for a table, list the table, followed by the columns to use as the message key. Each list entry takes the following format: \u003cfully-qualified_tableName\u003e:\u003ckeyColumn\u003e,\u003ckeyColumn\u003e To base a table key on multiple column names, insert commas between the column names. Each fully-qualified table name is a regular expression in the following format: \u003cdatabaseName\u003e.\u003ctableName\u003e The property can include entries for multiple tables. Use a semicolon to separate table entries in the list. The following example sets the message key for the tables inventory.customers and purchase.orders: inventory.customers:pk1,pk2;(.*).purchaseorders:pk3,pk4 For the table inventory.customer, the columns pk1 and pk2 are specified as the message key. For the purchaseorders tables in any database, the columns pk3 and pk4 server as the message key. There is no limit to the number of columns that you use to create custom message keys. However, it’s best to use the minimum number that are required to specify a unique key."
  },
  {
    "Key": "name",
    "Default value": "No default",
    "Description": "Unique name for the connector. If you attempt to use the same name to register multiple connectors, registration fails. This property is required by all Kafka Connect connectors."
  },
  {
    "Key": "schema.name.adjustment.mode",
    "Default value": "No default",
    "Description": "Specifies how the connector adjusts schema names for compatibility with the message converter used by the connector. Set one of the following options: none No adjustment. avro Replaces characters that are not valid in Avro names with underscore characters. avro_unicode Replaces underscore characters or characters that cannot be used in Avro names with corresponding unicode, such as _uxxxx. _ is an escape sequence, similar to a backslash in Java",
    "none": "No adjustment.",
    "avro": "Replaces characters that are not valid in Avro names with underscore characters.",
    "avro_unicode": "Replaces underscore characters or characters that cannot be used in Avro names with corresponding unicode, such as _uxxxx. _ is an escape sequence, similar to a backslash in Java"
  },
  {
    "Key": "skip.messages.without.change",
    "Default value": "false",
    "Description": "Specifies whether the connector emits messages for records when it does not detect a change in the included columns. Columns are considered to be included if they are listed in the column.include.list, or are not listed in the column.exclude.list. Set the value to true to prevent the connector from capturing records when no changes are present in the included columns."
  },
  {
    "Key": "table.exclude.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers of tables from which you do not want the connector to capture changes. The connector captures changes in any table that is not included in table.exclude.list. Each identifier is of the form databaseName.tableName. To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name. If you set this property, do not also set the table.include.list property."
  },
  {
    "Key": "table.include.list",
    "Default value": "empty string",
    "Description": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers of tables whose changes you want to capture. The connector does not capture changes in any table that is not included in table.include.list. Each identifier is of the form databaseName.tableName. By default, the connector captures changes in all non-system tables in every database from which it is configured to captures changes. To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name. If you set this property, do not also set the table.exclude.list property."
  },
  {
    "Key": "tasks.max",
    "Default value": "1",
    "Description": "The maximum number of tasks to create for this connector. Because the MySQL connector always uses a single task, changing the default value has no effect."
  },
  {
    "Key": "time.precision.mode",
    "Default value": "adaptive_time_microseconds",
    "Description": "Specifies the type of precision that the connector uses to represent time, date, and timestamps values. Set one of the following options: adaptive_time_microseconds The connector captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column’s type, with the exception of TIME type fields, which are always captured as microseconds. adaptive (deprecated) The connector captures time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the data type of the column. connect The connector always represents time and timestamp values using Kafka Connect’s built-in representations for Time, Date, and Timestamp, which use millisecond precision regardless of the database columns\u0027 precision.",
    "adaptive_time_microseconds": "The connector captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column’s type, with the exception of TIME type fields, which are always captured as microseconds.",
    "adaptive": "(deprecated) The connector captures time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the data type of the column.",
    "connect": "The connector always represents time and timestamp values using Kafka Connect’s built-in representations for Time, Date, and Timestamp, which use millisecond precision regardless of the database columns\u0027 precision."
  },
  {
    "Key": "tombstones.on.delete",
    "Default value": "true",
    "Description": "Specifies whether a delete event is followed by a tombstone event. After a source record is deleted, the connector can emit a tombstone event (the default behavior) to enable Kafka to completely delete all events that pertain to the key of the deleted row in case log compaction is enabled for the topic. Set one of the following options: true The connector represents delete operations by emitting a delete event and a subsequent tombstone event. false The connector emits only delete events.",
    "true": "The connector represents delete operations by emitting a delete event and a subsequent tombstone event.",
    "false": "The connector emits only delete events."
  },
  {
    "Key": "topic.prefix",
    "Default value": "No default",
    "Description": "A string that specifies the namespace for the MySQL database server or cluster from which Debezium captures changes. Because the topic prefix is used to name all of the Kafka topics that receive events that this connector emits, it’s important that the topic prefix is unique across all connectors. Values must contain only alphanumeric characters, hyphens, dots, and underscores. After you set this property, do not change its value. If you change the value, after the connector restarts, instead of continuing to emit events to the original topics, the connector emits subsequent events to topics whose names are based on the new value. The connector is also unable to recover its database schema history topic."
  },
  {
    "Key": "binlog.buffer.size",
    "Default value": "0",
    "Description": "The size of a look-ahead buffer used by the binlog reader. The default setting of 0 disables buffering. Under specific conditions, it is possible that the MySQL binlog contains uncommitted data finished by a ROLLBACK statement. Typical examples are using savepoints or mixing temporary and regular table changes in a single transaction. When a beginning of a transaction is detected then Debezium tries to roll forward the binlog position and find either COMMIT or ROLLBACK so it can determine whether to stream the changes from the transaction. The size of the binlog buffer defines the maximum number of changes in the transaction that Debezium can buffer while searching for transaction boundaries. If the size of the transaction is larger than the buffer then Debezium must rewind and re-read the events that have not fit into the buffer while streaming. This feature is incubating. Feedback is encouraged. It is expected that this feature is not completely polished."
  },
  {
    "Key": "connect.keep.alive",
    "Default value": "true",
    "Description": "A Boolean value that specifies whether a separate thread should be used to ensure that the connection to the MySQL server or cluster is kept alive."
  },
  {
    "Key": "converters",
    "Default value": "No default",
    "Description": "Enumerates a comma-separated list of the symbolic names of the custom converter instances that the connector can use. For example, boolean. This property is required to enable the connector to use a custom converter. For each converter that you configure for a connector, you must also add a .type property, which specifies the fully-qualified name of the class that implements the converter interface. The .type property uses the following format: \u003cconverterSymbolicName\u003e.type For example, boolean.type: io.debezium.connector.binlog.converters.TinyIntOneToBooleanConverter If you want to further control the behavior of a configured converter, you can add one or more configuration parameters to pass values to the converter. To associate these additional configuration parameter with a converter, prefix the parameter name with the symbolic name of the converter. For example, to define a selector parameter that specifies the subset of columns that the boolean converter processes, add the following property: boolean.selector\u003ddb1.table1.*, db1.table2.column1"
  },
  {
    "Key": "custom.metric.tags",
    "Default value": "No default",
    "Description": "Defines tags that customize MBean object names by adding metadata that provides contextual information. Specify a comma-separated list of key-value pairs. Each key represents a tag for the MBean object name, and the corresponding value represents a value for the key, for example, k1\u003dv1,k2\u003dv2. The connector appends the specified tags to the base MBean object name. Tags can help you to organize and categorize metrics data. You can define tags to identify particular application instances, environments, regions, versions, and so forth. For more information, see Customized MBean names."
  },
  {
    "Key": "database.initial.statements",
    "Default value": "No default",
    "Description": "A semicolon separated list of SQL statements to be executed when a JDBC connection, not the connection that is reading the transaction log, to the database is established. To specify a semicolon as a character in a SQL statement and not as a delimiter, use two semicolons, (;;). The connector might establish JDBC connections at its own discretion, so this property is ony for configuring session parameters. It is not for executing DML statements."
  },
  {
    "Key": "database.query.timeout.ms",
    "Default value": "600000 (10 minutes)",
    "Description": "Specifies the time, in milliseconds, that the connector waits for a query to complete. Set the value to 0 (zero) to remove the timeout limit."
  },
  {
    "Key": "database.ssl.keystore",
    "Default value": "No default",
    "Description": "An optional setting that specifies the location of the key store file. A key store file can be used for two-way authentication between the client and the MySQL server."
  },
  {
    "Key": "database.ssl.keystore.password",
    "Default value": "No default",
    "Description": "The password for the key store file. Specify a password only if the database.ssl.keystore is configured."
  },
  {
    "Key": "database.ssl.mode",
    "Default value": "preferred",
    "Description": "Specifies whether the connector uses an encrypted connection. The following settings are available: disabled Specifies the use of an unencrypted connection. preferred The connector establishes an encrypted connection if the server supports secure connections. If the server does not support secure connections, the connector falls back to using an unencrypted connection. required The connector establishes an encrypted connection. If it is unable to establish an encrypted connection, the connector fails. verify_ca The connector behaves as when you set the required option, but it also verifies the server TLS certificate against the configured Certificate Authority (CA) certificates. If the server TLS certificate does not match any valid CA certificates, the connector fails. verify_identity The connector behaves as when you set the verify_ca option, but it also verifies that the server certificate matches the host of the remote connection.",
    "disabled": "Specifies the use of an unencrypted connection.",
    "preferred": "The connector establishes an encrypted connection if the server supports secure connections. If the server does not support secure connections, the connector falls back to using an unencrypted connection.",
    "required": "The connector establishes an encrypted connection. If it is unable to establish an encrypted connection, the connector fails.",
    "verify_ca": "The connector behaves as when you set the required option, but it also verifies the server TLS certificate against the configured Certificate Authority (CA) certificates. If the server TLS certificate does not match any valid CA certificates, the connector fails.",
    "verify_identity": "The connector behaves as when you set the verify_ca option, but it also verifies that the server certificate matches the host of the remote connection."
  },
  {
    "Key": "database.ssl.truststore",
    "Default value": "No default",
    "Description": "The location of the trust store file for the server certificate verification."
  },
  {
    "Key": "database.ssl.truststore.password",
    "Default value": "No default",
    "Description": "The password for the trust store file. Used to check the integrity of the truststore, and unlock the truststore."
  },
  {
    "Key": "enable.time.adjuster",
    "Default value": "true",
    "Description": "Boolean value that indicates whether the connector converts a 2-digit year specification to 4 digits. Set the value to false when conversion is fully delegated to the database. MySQL users can insert year values with either 2-digits or 4-digits. 2-digit values are mapped to a year in the range 1970 - 2069. By default, the connector performs the conversion."
  },
  {
    "Key": "errors.max.retries",
    "Default value": "-1",
    "Description": "Specifies how the connector responds after an operation that results in a retriable error, such as a connection error. Set one of the following options: -1 No limit. The connector always restarts automatically, and retries the operation, regardless of the number of previous failures. 0 Disabled. The connector fails immediately, and never retries the operation. User intervention is required to restart the connector. \u003e0 The connector restarts automatically until it reaches the specified maximum number of retries. After the next failure, the connector stops, and user intervention is required to restart it.",
    "-1": "No limit. The connector always restarts automatically, and retries the operation, regardless of the number of previous failures.",
    "0": "Disabled. The connector fails immediately, and never retries the operation. User intervention is required to restart the connector.",
    "\u003e0": "The connector restarts automatically until it reaches the specified maximum number of retries. After the next failure, the connector stops, and user intervention is required to restart it."
  },
  {
    "Key": "event.converting.failure.handling.mode",
    "Default value": "warn",
    "Description": "Specifies how the connector responds when it cannot convert a table record due to a mismatch between the data type of a column and the type specified by the Debezium internal schema. Set one of the following options: fail An exception reports that conversion failed because the data type of the field did not match the schema type, and indicates that it might be necessary to restart the connector in schema _only_recovery mode to enable a successful conversion. warn The connector writes a null value to the event field for the column that failed conversion, writes a message to the warning log . skip The connector writes a null value to the event field for the column that failed conversion, and writes a message to the debug log.",
    "fail": "An exception reports that conversion failed because the data type of the field did not match the schema type, and indicates that it might be necessary to restart the connector in schema _only_recovery mode to enable a successful conversion.",
    "warn": "The connector writes a null value to the event field for the column that failed conversion, writes a message to the warning log .",
    "skip": "The connector writes a null value to the event field for the column that failed conversion, and writes a message to the debug log."
  },
  {
    "Key": "event.processing.failure.handling.mode",
    "Default value": "fail",
    "Description": "Specifies how the connector handles failures that occur when processing events, for example, if it encounters a corrupted event. The following settings are available: fail The connector raises an exception that reports the problematic event and its position. The connector then stops. warn The connector does not raise an exception. Instead, it logs the problematic event and its position, and then skips the event. ignore The connector ignores the problematic event, and does not generate a log entry.",
    "fail": "The connector raises an exception that reports the problematic event and its position. The connector then stops.",
    "warn": "The connector does not raise an exception. Instead, it logs the problematic event and its position, and then skips the event.",
    "ignore": "The connector ignores the problematic event, and does not generate a log entry."
  },
  {
    "Key": "heartbeat.action.query",
    "Default value": "No default",
    "Description": "Specifies a query that the connector executes on the source database when the connector sends a heartbeat message. For example, the following query periodically captures the state of the executed GTID set in the source database. INSERT INTO gtid_history_table (select @gtid_executed)"
  },
  {
    "Key": "heartbeat.interval.ms",
    "Default value": "0",
    "Description": "Specifies how frequently the connector sends heartbeat messages to a Kafka topic. By default, the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages."
  },
  {
    "Key": "incremental.snapshot.allow.schema.changes",
    "Default value": "false",
    "Description": "Specifies whether the connector allows schema changes during an incremental snapshot. When the value is set to true, the connector detects schema change during an incremental snapshot, and re-select a current chunk to avoid locking DDLs. Changes to a primary key are not supported. Changing the primary during an incremental snapshot, can lead to incorrect results. A further limitation is that if a schema change affects only the default values of columns, then the change is not detected until the DDL is processed from the binlog stream. This does not affect the values of snapshot events, but the schema of these snapshot events may have outdated defaults."
  },
  {
    "Key": "incremental.snapshot.chunk.size",
    "Default value": "1024",
    "Description": "The maximum number of rows that the connector fetches and reads into memory when it retrieves an incremental snapshot chunk. Increasing the chunk size provides greater efficiency, because the snapshot runs fewer snapshot queries of a greater size. However, larger chunk sizes also require more memory to buffer the snapshot data. Adjust the chunk size to a value that provides the best performance in your environment."
  },
  {
    "Key": "incremental.snapshot.watermarking.strategy",
    "Default value": "insert_insert",
    "Description": "Specifies the watermarking mechanism that the connector uses during an incremental snapshot to deduplicate events that might be captured by an incremental snapshot and then recaptured after streaming resumes. You can specify one of the following options: insert_insert (default) When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads during the snapshot, it writes an entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, Debezium inserts a second entry that records the signal to close the window. insert_delete When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads, it writes a single entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, this entry is removed. No entry is created for the signal to close the snapshot window. Set this option to prevent rapid growth of the signaling data collection.",
    "insert_insert (default)": "When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads during the snapshot, it writes an entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, Debezium inserts a second entry that records the signal to close the window.",
    "insert_delete": "When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads, it writes a single entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, this entry is removed. No entry is created for the signal to close the snapshot window. Set this option to prevent rapid growth of the signaling data collection."
  },
  {
    "Key": "max.batch.size",
    "Default value": "2048",
    "Description": "Positive integer value that specifies the maximum size of each batch of events that should be processed during each iteration of this connector."
  },
  {
    "Key": "max.queue.size",
    "Default value": "8192",
    "Description": "A positive integer value that specifies the maximum number of records that the blocking queue can hold. When Debezium reads events streamed from the database, it places the events in the blocking queue before it writes them to Kafka. The blocking queue can provide backpressure for reading change events from the database in cases where the connector ingests messages faster than it can write them to Kafka, or when Kafka becomes unavailable. Events that are held in the queue are disregarded when the connector periodically records offsets. Always set max.queue.size to a value that is larger than the value of max.batch.size."
  },
  {
    "Key": "max.queue.size.in.bytes",
    "Default value": "0",
    "Description": "A long integer value that specifies the maximum volume of the blocking queue in bytes. By default, volume limits are not specified for the blocking queue. To specify the number of bytes that the queue can consume, set this property to a positive long value. If max.queue.size is also set, writing to the queue is blocked when the size of the queue reaches the limit specified by either property. For example, if you set max.queue.size\u003d1000, and max.queue.size.in.bytes\u003d5000, writing to the queue is blocked after the queue contains 1000 records, or after the volume of the records in the queue reaches 5000 bytes."
  },
  {
    "Key": "min.row.count.to.stream.results",
    "Default value": "1000",
    "Description": "During a snapshot, the connector queries each table for which the connector is configured to capture changes. The connector uses each query result to produce a read event that contains data for all rows in that table. This property determines whether the MySQL connector puts results for a table into memory, which is fast but requires large amounts of memory, or streams the results, which can be slower but work for very large tables. The setting of this property specifies the minimum number of rows a table must contain before the connector streams results. To skip all table size checks and always stream all results during a snapshot, set this property to 0."
  },
  {
    "Key": "notification.enabled.channels",
    "Default value": "No default",
    "Description": "List of notification channel names that are enabled for the connector. By default, the following channels are available: sink log jmx"
  },
  {
    "Key": "poll.interval.ms",
    "Default value": "500 (0.5 seconds)",
    "Description": "Positive integer value that specifies the number of milliseconds the connector waits for new change events to appear before it starts processing a batch of events."
  },
  {
    "Key": "provide.transaction.metadata",
    "Default value": "false",
    "Description": "Determines whether the connector generates events with transaction boundaries and enriches change event envelopes with transaction metadata. Specify true if you want the connector to do this. For more information, see Transaction metadata."
  },
  {
    "Key": "read.only",
    "Default value": "false",
    "Description": "Specifies whether a connector writes watermarks to the signal data collection to track the progress of an incremental snapshot. Set the value to true to enable a connector that has a read-only connection to the database to use an incremental snapshot watermarking strategy that does not require writing to the signal data collection."
  },
  {
    "Key": "signal.data.collection",
    "Default value": "No default",
    "Description": "Fully-qualified name of the data collection that is used to send signals to the connector. Use the following format to specify the collection name: \u003cdatabaseName\u003e.\u003ctableName\u003e"
  },
  {
    "Key": "signal.enabled.channels",
    "Default value": "No default",
    "Description": "List of the signaling channel names that are enabled for the connector. By default, the following channels are available: source kafka file jmx"
  },
  {
    "Key": "skipped.operations",
    "Default value": "t",
    "Description": "A comma-separated list of the operation types that you want the connector to skip during streaming. Set one of the following options to specify the operations to skip: c Insert/create operations. u Update operations. d Delete operations. t Truncate operations. none The connector does not skip any operations.",
    "c": "Insert/create operations.",
    "u": "Update operations.",
    "d": "Delete operations.",
    "t": "Truncate operations.",
    "none": "The connector does not skip any operations."
  },
  {
    "Key": "snapshot.delay.ms",
    "Default value": "No default",
    "Description": "An interval in milliseconds that the connector should wait before performing a snapshot when the connector starts. If you are starting multiple connectors in a cluster, this property is useful for avoiding snapshot interruptions, which might cause re-balancing of connectors."
  },
  {
    "Key": "snapshot.fetch.size",
    "Default value": "Unset",
    "Description": "By default, during a snapshot, the connector reads table content in batches of rows. Set this property to specify the maximum number of rows in a batch. To maintain connector performance, it’s best to preserve the unset default of this property. This default configuration enables MySQL to stream the result set to Debezium one row at a time. By contrast, if you set this property, performance problems can result, because Debezium attempts to fetch the entire result set into memory at once."
  },
  {
    "Key": "snapshot.include.collection.list",
    "Default value": "All tables specified in the table.include.list.",
    "Description": "An optional, comma-separated list of regular expressions that match the fully-qualified names (\u003cdatabaseName\u003e.\u003ctableName\u003e) of the tables to include in a snapshot. The specified items must be named in the connector’s table.include.list property. This property takes effect only if the connector’s snapshot.mode property is set to a value other than never. This property does not affect the behavior of incremental snapshots. To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name."
  },
  {
    "Key": "snapshot.lock.timeout.ms",
    "Default value": "10000",
    "Description": "Positive integer that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If the connector cannot acquire table locks in this time interval, the snapshot fails. For more information, see the documentation that describes how MySQL connectors perform database snapshots."
  },
  {
    "Key": "snapshot.locking.mode",
    "Default value": "minimal",
    "Description": "Specifies whether and for how long the connector holds the global MySQL read lock, which prevents any updates to the database while the connector is performing a snapshot. The following settings are available: minimal The connector holds the global read lock for only the initial phase of the snapshot during which it reads the database schemas and other metadata. During the next phase of the snapshot, the connector releases the lock as it selects all rows from each table. To perform the SELECT operation in a consistent fashion, the connector uses a REPEATABLE READ transaction. Although the release of the global read lock permits other MySQL clients to update the database, use of REPEATABLE READ isolation ensures a consistent snapshot, because the connector continues to read the same data for the duration of the transaction. extended Blocks all write operations for the duration of the snapshot. Use this setting if clients submit concurrent operations that are incompatible with the REPEATABLE READ isolation level in MySQL. none Prevents the connector from acquiring any table locks during the snapshot. Although this option is allowed with all snapshot modes, it is safe to use only if no schema changes occur while the snapshot is running. Tables that are defined with the MyISAM engine always acquire a table lock. As a result, such tables are locked even if you set this option. This behavior differs from tables that are defined by the InnoDB engine, which acquire row-level locks. custom The connector performs a snapshot according to the implementation specified by the snapshot.locking.mode.custom.name property, which is a custom implementation of the io.debezium.spi.snapshot.SnapshotLock interface.",
    "minimal": "The connector holds the global read lock for only the initial phase of the snapshot during which it reads the database schemas and other metadata. During the next phase of the snapshot, the connector releases the lock as it selects all rows from each table. To perform the SELECT operation in a consistent fashion, the connector uses a REPEATABLE READ transaction. Although the release of the global read lock permits other MySQL clients to update the database, use of REPEATABLE READ isolation ensures a consistent snapshot, because the connector continues to read the same data for the duration of the transaction.",
    "extended": "Blocks all write operations for the duration of the snapshot. Use this setting if clients submit concurrent operations that are incompatible with the REPEATABLE READ isolation level in MySQL.",
    "none": "Prevents the connector from acquiring any table locks during the snapshot. Although this option is allowed with all snapshot modes, it is safe to use only if no schema changes occur while the snapshot is running. Tables that are defined with the MyISAM engine always acquire a table lock. As a result, such tables are locked even if you set this option. This behavior differs from tables that are defined by the InnoDB engine, which acquire row-level locks.",
    "custom": "The connector performs a snapshot according to the implementation specified by the snapshot.locking.mode.custom.name property, which is a custom implementation of the io.debezium.spi.snapshot.SnapshotLock interface."
  },
  {
    "Key": "snapshot.locking.mode.custom.name",
    "Default value": "No default",
    "Description": "When snapshot.locking.mode is set to custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the \u0027io.debezium.spi.snapshot.SnapshotLock\u0027 interface. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.max.threads",
    "Default value": "1",
    "Description": "Specifies the number of threads that the connector uses when performing an initial snapshot. To enable parallel initial snapshots, set the property to a value greater than 1. In a parallel initial snapshot, the connector processes multiple tables concurrently. Parallel initial snapshots is an incubating feature."
  },
  {
    "Key": "snapshot.mode",
    "Default value": "initial",
    "Description": "Specifies the criteria for running a snapshot when the connector starts. The following settings are available: always The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts. initial The connector runs a snapshot only when no offsets have been recorded for the logical server name, or if it detects that an earlier snapshot failed to complete. After the snapshot completes, the connector begins to stream event records for subsequent database changes. initial_only The connector runs a snapshot only when no offsets have been recorded for the logical server name. After the snapshot completes, the connector stops. It does not transition to streaming to read change events from the binlog. schema_only Deprecated, see no_data. no_data The connector runs a snapshot that captures only the schema, but not any table data. Set this option if you do not need the topics to contain a consistent snapshot of the data, but you want to capture any schema changes that were applied after the last connector restart. schema_only_recovery Deprecated, see recovery. recovery Set this option to restore a database schema history topic that is lost or corrupted. After a restart, the connector runs a snapshot that rebuilds the topic from the source tables. You can also set the property to periodically prune a database schema history topic that experiences unexpected growth. Do not use this mode to perform a snapshot if schema changes were committed to the database after the last connector shutdown. never When the connector starts, rather than performing a snapshot, it immediately begins to stream event records for subsequent database changes. This option is under consideration for future deprecation, in favor of the no_data option. when_needed After the connector starts, it performs a snapshot only if it detects one of the following circumstances: It cannot detect any topic offsets. A previously recorded offset specifies a binlog position or GTID that is not available on the server. configuration_based With this option, you control snapshot behavior through a set of connector properties that have the prefix \u0027snapshot.mode.configuration.based\u0027. custom The connector performs a snapshot according to the implementation specified by the snapshot.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.Snapshotter interface.",
    "always": "The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts.",
    "initial": "The connector runs a snapshot only when no offsets have been recorded for the logical server name, or if it detects that an earlier snapshot failed to complete. After the snapshot completes, the connector begins to stream event records for subsequent database changes.",
    "initial_only": "The connector runs a snapshot only when no offsets have been recorded for the logical server name. After the snapshot completes, the connector stops. It does not transition to streaming to read change events from the binlog.",
    "schema_only": "Deprecated, see no_data.",
    "no_data": "The connector runs a snapshot that captures only the schema, but not any table data. Set this option if you do not need the topics to contain a consistent snapshot of the data, but you want to capture any schema changes that were applied after the last connector restart.",
    "schema_only_recovery": "Deprecated, see recovery.",
    "recovery": "Set this option to restore a database schema history topic that is lost or corrupted. After a restart, the connector runs a snapshot that rebuilds the topic from the source tables. You can also set the property to periodically prune a database schema history topic that experiences unexpected growth. Do not use this mode to perform a snapshot if schema changes were committed to the database after the last connector shutdown.",
    "never": "When the connector starts, rather than performing a snapshot, it immediately begins to stream event records for subsequent database changes. This option is under consideration for future deprecation, in favor of the no_data option.",
    "when_needed": "After the connector starts, it performs a snapshot only if it detects one of the following circumstances: It cannot detect any topic offsets. A previously recorded offset specifies a binlog position or GTID that is not available on the server.",
    "configuration_based": "With this option, you control snapshot behavior through a set of connector properties that have the prefix \u0027snapshot.mode.configuration.based\u0027.",
    "custom": "The connector performs a snapshot according to the implementation specified by the snapshot.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.Snapshotter interface."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.data",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes table data when it performs a snapshot."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.on.data.error",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes table data in a snapshot in the event that data is no longer available in the transaction log."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.on.schema.error",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes table schema in a snapshot if the schema history topic is not available."
  },
  {
    "Key": "snapshot.mode.configuration.based.snapshot.schema",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector includes the table schema when it performs a snapshot."
  },
  {
    "Key": "snapshot.mode.configuration.based.start.stream",
    "Default value": "false",
    "Description": "If the snapshot.mode is set to configuration_based, set this property to specify whether the connector begins to stream change events after a snapshot completes."
  },
  {
    "Key": "snapshot.mode.custom.name",
    "Default value": "No default",
    "Description": "If snapshot.mode is set to custom, use this setting to specify the name of the custom implementation that is provided in the name() method that is defined in the \u0027io.debezium.spi.snapshot.Snapshotter\u0027 interface. After a connector restart, Debezium calls the specified custom implementation to determine whether to perform a snapshot. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.query.mode",
    "Default value": "select_all",
    "Description": "Specifies how the connector queries data while performing a snapshot. Set one of the following options: select_all (default) The connector uses a select all query to retrieve rows from captured tables, optionally adjusting the columns selected based on the column include and exclude list configurations. custom The connector performs a snapshot query according to the implementation specified by the snapshot.query.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.SnapshotQuery interface. This setting enables you to manage snapshot content in a more flexible manner compared to using the snapshot.select.statement.overrides property.",
    "select_all (default)": "The connector uses a select all query to retrieve rows from captured tables, optionally adjusting the columns selected based on the column include and exclude list configurations.",
    "custom": "The connector performs a snapshot query according to the implementation specified by the snapshot.query.mode.custom.name property, which defines a custom implementation of the io.debezium.spi.snapshot.SnapshotQuery interface. This setting enables you to manage snapshot content in a more flexible manner compared to using the snapshot.select.statement.overrides property."
  },
  {
    "Key": "snapshot.query.mode.custom.name",
    "Default value": "No default",
    "Description": "When snapshot.query.mode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the \u0027io.debezium.spi.snapshot.SnapshotQuery\u0027 interface. For more information, see custom snapshotter SPI."
  },
  {
    "Key": "snapshot.select.statement.overrides",
    "Default value": "No default",
    "Description": "Specifies the table rows to include in a snapshot. Use the property if you want a snapshot to include only a subset of the rows in a table. This property affects snapshots only. It does not apply to events that the connector reads from the log. The property contains a comma-separated list of fully-qualified table names in the form \u003cdatabaseName\u003e.\u003ctableName\u003e. For example, \"snapshot.select.statement.overrides\": \"inventory.products,customers.orders\" For each table in the list, add a further configuration property that specifies the SELECT statement for the connector to run on the table when it takes a snapshot. The specified SELECT statement determines the subset of table rows to include in the snapshot. Use the following format to specify the name of this SELECT statement property: snapshot.select.statement.overrides.\u003cdatabaseName\u003e.\u003ctableName\u003e For example, snapshot.select.statement.overrides.customers.orders From a customers.orders table that includes the soft-delete column, delete_flag, add the following properties if you want a snapshot to include only those records that are not soft-deleted: \"snapshot.select.statement.overrides\": \"customer.orders\",\n\"snapshot.select.statement.overrides.customer.orders\": \"SELECT * FROM [customers].[orders] WHERE delete_flag \u003d 0 ORDER BY id DESC\" In the resulting snapshot, the connector includes only the records for which delete_flag \u003d 0."
  },
  {
    "Key": "snapshot.tables.order.by.row.count",
    "Default value": "disabled",
    "Description": "Specifies the order in which the connector processes tables when it performs an initial snapshot. Set one of the following options: descending The connector snapshots tables in order, based on the number of rows from the highest to the lowest. ascending The connector snapshots tables in order, based on the number of rows, from lowest to highest. disabled The connector disregards row count when performing an initial snapshot.",
    "descending": "The connector snapshots tables in order, based on the number of rows from the highest to the lowest.",
    "ascending": "The connector snapshots tables in order, based on the number of rows, from lowest to highest.",
    "disabled": "The connector disregards row count when performing an initial snapshot."
  },
  {
    "Key": "source.struct.version",
    "Default value": "v2",
    "Description": "Schema version for the source block in Debezium events. Debezium 0.10 introduced a few breaking changes to the structure of the source block in order to unify the exposed structure across all the connectors. By setting this option to v1, the structure used in earlier versions can be produced. However, this setting is not recommended and is planned for removal in a future Debezium version."
  },
  {
    "Key": "streaming.delay.ms",
    "Default value": "0",
    "Description": "Specifies the time, in milliseconds, that the connector delays the start of the streaming process after it completes a snapshot. Setting a delay interval helps to prevent the connector from restarting snapshots in the event that a failure occurs immediately after the snapshot completes, but before the streaming process begins. Set a delay value that is higher than the value of the offset.flush.interval.ms property that is set for the Kafka Connect worker."
  },
  {
    "Key": "table.ignore.builtin",
    "Default value": "true",
    "Description": "A Boolean value that specifies whether built-in system tables should be ignored. This applies regardless of the table include and exclude lists. By default, changes that occur to the values in system tables are excluded from capture, and Debezium does not generate events for system table changes."
  },
  {
    "Key": "topic.cache.size",
    "Default value": "10000",
    "Description": "Specifies the number of topic names that can be stored in memory in a bounded concurrent hash map. The connector uses the cache to help determine the topic name that corresponds to a data collection."
  },
  {
    "Key": "topic.delimiter",
    "Default value": ". (period)",
    "Description": "Specifies the delimiter that the connector inserts between components of the topic name."
  },
  {
    "Key": "topic.heartbeat.prefix",
    "Default value": "__debezium-heartbeat",
    "Description": "Specifies the name of the topic to which the connector sends heartbeat messages. The topic name takes the following format: topic.heartbeat.prefix.topic.prefix For example, when you set the default value for this property, and the topic prefix is fulfillment, the topic name is __debezium-heartbeat.fulfillment."
  },
  {
    "Key": "topic.naming.strategy",
    "Default value": "io.debezium.schema.DefaultTopicNamingStrategy",
    "Description": "The name of the TopicNamingStrategy class that the connector uses. The specified strategy determines how the connector names the topics that store event records for data changes, schema changes, transactions, heartbeats, and so forth."
  },
  {
    "Key": "topic.transaction",
    "Default value": "transaction",
    "Description": "Specifies the name of the topic to which the connector sends transaction metadata messages. The topic name takes the following pattern: topic.prefix.topic.transaction For example, if the topic prefix is fulfillment, the default topic name is fulfillment.transaction."
  },
  {
    "Key": "use.nongraceful.disconnect",
    "Default value": "false",
    "Description": "A Boolean value that specifies whether the binary log client’s keepalive thread sets the SO_LINGER socket option to 0 to immediately close stale TCP connections. Set the value to true if the connector experiences deadlocks in SSLSocketImpl.close. For more information, see Issue 133 in the mysql-binlog-connector-java GitHub repository."
  },
  {
    "Key": "schema.history.internal.kafka.topic",
    "Default value": "No default",
    "Description": "The full name of the Kafka topic where the connector stores the database schema history."
  },
  {
    "Key": "schema.history.internal.kafka.bootstrap.servers",
    "Default value": "No default",
    "Description": "A list of host/port pairs that the connector uses for establishing an initial connection to the Kafka cluster. This connection is used for retrieving the database schema history previously stored by the connector, and for writing each DDL statement read from the source database. Each pair should point to the same Kafka cluster used by the Kafka Connect process."
  },
  {
    "Key": "schema.history.internal.kafka.recovery.poll.interval.ms",
    "Default value": "100",
    "Description": "An integer value that specifies the maximum number of milliseconds the connector should wait during startup/recovery while polling for persisted data. The default is 100ms."
  },
  {
    "Key": "schema.history.internal.kafka.query.timeout.ms",
    "Default value": "3000",
    "Description": "An integer value that specifies the maximum number of milliseconds the connector should wait while fetching cluster information using Kafka admin client."
  },
  {
    "Key": "schema.history.internal.kafka.create.timeout.ms",
    "Default value": "30000",
    "Description": "An integer value that specifies the maximum number of milliseconds the connector should wait while create kafka history topic using Kafka admin client."
  },
  {
    "Key": "schema.history.internal.kafka.recovery.attempts",
    "Default value": "100",
    "Description": "The maximum number of times that the connector should try to read persisted history data before the connector recovery fails with an error. The maximum amount of time to wait after receiving no data is recovery.attempts × recovery.poll.interval.ms."
  },
  {
    "Key": "schema.history.internal.skip.unparseable.ddl",
    "Default value": "false",
    "Description": "A Boolean value that specifies whether the connector should ignore malformed or unknown database statements or stop processing so a human can fix the issue. The safe default is false. Skipping should be used only with care as it can lead to data loss or mangling when the binlog is being processed."
  },
  {
    "Key": "schema.history.internal.store.only.captured.tables.ddl",
    "Default value": "false",
    "Description": "A Boolean value that specifies whether the connector records schema structures from all tables in a schema or database, or only from tables that are designated for capture. Specify one of the following values: false (default) During a database snapshot, the connector records the schema data for all non-system tables in the database, including tables that are not designated for capture. It’s best to retain the default setting. If you later decide to capture changes from tables that you did not originally designate for capture, the connector can easily begin to capture data from those tables, because their schema structure is already stored in the schema history topic. Debezium requires the schema history of a table so that it can identify the structure that was present at the time that a change event occurred. true During a database snapshot, the connector records the table schemas only for the tables from which Debezium captures change events. If you change the default value, and you later configure the connector to capture data from other tables in the database, the connector lacks the schema information that it requires to capture change events from the tables."
  },
  {
    "Key": "schema.history.internal.store.only.captured.databases.ddl",
    "Default value": "false",
    "Description": "A Boolean value that specifies whether the connector records schema structures from all logical databases in the database instance. Specify one of the following values: true The connector records schema structures only for tables in the logical database and schema from which Debezium captures change events. false The connector records schema structures for all logical databases."
  },
  {
    "Key": "signal.kafka.topic",
    "Default value": "\u003ctopic.prefix\u003e-signal",
    "Description": "The name of the Kafka topic that the connector monitors for ad hoc signals. If automatic topic creation is disabled, you must manually create the required signaling topic. A signaling topic is required to preserve signal ordering. The signaling topic must have a single partition."
  },
  {
    "Key": "signal.kafka.groupId",
    "Default value": "kafka-signal",
    "Description": "The name of the group ID that is used by Kafka consumers."
  },
  {
    "Key": "signal.kafka.bootstrap.servers",
    "Default value": "No default",
    "Description": "A list of the host and port pairs that the connector uses to establish its initial connection to the Kafka cluster. Each pair references the Kafka cluster that is used by the Debezium Kafka Connect process."
  },
  {
    "Key": "signal.kafka.poll.timeout.ms",
    "Default value": "100",
    "Description": "An integer value that specifies the maximum number of milliseconds that the connector waits when polling signals."
  },
  {
    "Key": "notification.sink.topic.name",
    "Default value": "No default",
    "Description": "The name of the topic that receives notifications from Debezium. This property is required when you configure the notification.enabled.channels property to include sink as one of the enabled notification channels."
  }
]
